<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://zhangyaoo.github.io</id>
    <title>will</title>
    <updated>2020-10-23T09:12:11.881Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://zhangyaoo.github.io"/>
    <link rel="self" href="https://zhangyaoo.github.io/atom.xml"/>
    <subtitle>生死看淡，不服就干</subtitle>
    <logo>https://zhangyaoo.github.io/images/avatar.png</logo>
    <icon>https://zhangyaoo.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, will</rights>
    <entry>
        <title type="html"><![CDATA[采用分布式ID解决分库分表踩的坑]]></title>
        <id>https://zhangyaoo.github.io/post/ji-yi-ci-cai-yong-fen-bu-shi-id-jie-jue-fen-ku-fen-biao-cai-de-keng/</id>
        <link href="https://zhangyaoo.github.io/post/ji-yi-ci-cai-yong-fen-bu-shi-id-jie-jue-fen-ku-fen-biao-cai-de-keng/">
        </link>
        <updated>2020-10-16T02:55:39.000Z</updated>
        <content type="html"><![CDATA[<h3 id="背景">背景</h3>
<h3 id=""></h3>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Saas系统多数据源路由优雅解决方案]]></title>
        <id>https://zhangyaoo.github.io/post/saas-xi-tong-duo-shu-ju-yuan-lu-you-you-ya-jie-jue-fang-an/</id>
        <link href="https://zhangyaoo.github.io/post/saas-xi-tong-duo-shu-ju-yuan-lu-you-you-ya-jie-jue-fang-an/">
        </link>
        <updated>2020-10-16T02:26:30.000Z</updated>
        <content type="html"><![CDATA[<h2 id="背景">背景</h2>
<p>在目前的SaaS系统中，业务开发者需要重点关注的一个问题就是数据隔离问题，这个是做SaaS系统必须要考虑的点，多租户数据隔离是每个SaaS系统都要遇到并且要解决的问题，笔者就分享下解决这种问题的思路、具体的解决方案以及优雅的解决思路。</p>
<h2 id="一-解决方案介绍">一、解决方案介绍</h2>
<h3 id="目前业界数据隔离方案">目前业界数据隔离方案</h3>
<p>1、独立数据库，通过动态切换数据源来实现多租户<br>
2、共享数据库，隔离数据架构<br>
3、共享数据库，共享数据表，使用字段来区分不同租户，此方案成本最低</p>
<p>以上方案从上到下，安全性逐渐降低。由于考虑到安全问题，故采用第一种方案解决数据隔离<br>
优点：</p>
<ol>
<li>非常安全</li>
<li>数据互不影响，性能互不影响</li>
<li>数据迁移，数据扩展方便</li>
</ol>
<p>缺点：</p>
<ol>
<li>需要维护大量的数据库</li>
<li>需要自行切换数据库，开发量多且实现复杂</li>
</ol>
<h3 id="具体技术实现">具体技术实现</h3>
<p><strong>简单的架构图</strong><br>
<img src="https://zhangyaoo.github.io/post-images/1603179800053.png" alt="" loading="lazy"><br>
如图所示，SaaS项目大概架构图，关键点是应用层传参，以及路由层的实现。</p>
<p><strong>实现</strong><br>
1、应用层：项目中应用service层是dubbo服务，而且项目分多层，这里需要考虑到多层服务场景下，如何优雅传参问题，如下图所示<br>
<img src="https://zhangyaoo.github.io/post-images/1603181882775.png" alt="" loading="lazy"><br>
我们考虑到租户ID是唯一标识，和业务参数绑定在一起不优雅，所以两种参数分开处理，业务参数直接参数透传，租户ID唯一标识通过隐式传参来处理（参考dubbo http://dubbo.apache.org/zh-cn/docs/user/demos/attachment.html），并且参数记录到服务本地的threadlocal中，以便后续其他业务需要。具体实现如下：<br>
<img src="https://zhangyaoo.github.io/post-images/1603183675071.png" alt="" loading="lazy"></p>
<p>2、路由层：路由层实现主要是自行实现spring框架中DataSource接口，自定义dynamicDataSource类，然后implement DataSource接口，实现getConnection方法。然后重新定义SqlSessionFactory的bean，将自定义DataSource类属性注入。<br>
<img src="https://zhangyaoo.github.io/post-images/1603183148592.png" alt="" loading="lazy"><br>
<img src="https://zhangyaoo.github.io/post-images/1603183156251.png" alt="" loading="lazy"><br>
然后我们只需要关注getConnection方法根据租户ID，选择相对应的租户连接池就可以了。<br>
如图中，我们只需要实现这个selectTenantCodeDataSource()这个方法就可以了，这个方法实现很简单，这里就不贴图了。selectTenantCodeDataSource()方法主要就是从threadlocal中拿租户ID，然后去缓存池map中拿出连接池信息。<br>
<img src="https://zhangyaoo.github.io/post-images/1603183509071.png" alt="" loading="lazy"><br>
其中，dataSourceCachePool是在初始化配置时候，将所有的租户连接池直接创建，然后扔到dataSourceCachePool。key是租户的ID，value是连接池信息。</p>
<p>具体的初始化配置：</p>
<pre><code class="language-java">/**
 * 初始化数据源
 */
@Configuration
public class DataSourceInit {
    
    @PostConstruct
    public void InitDataSource()  {
        log.info(&quot;=====初始化数据源=====&quot;);
        TenantRoutingDataSource tenantRoutingDataSource = (TenantRoutingDataSource)ApplicationContextProvider.getBean(&quot;tenantRoutingDataSource&quot;);
        Map&lt;String, DataSourceCache&gt; dataSourceCachePool = new HashMap&lt;&gt;();

        List&lt;TenantInfo&gt; tenantList = tenantInfoService.InitTenantInfo();
        for (TenantInfo tenantInfo : tenantList) {
            log.info(tenantInfo.toString());
            HikariDataSource dataSource = new HikariDataSource();
            dataSource.setDriverClassName(tenantInfo.getDatasourceDriver());
            dataSource.setJdbcUrl(tenantInfo.getDatasourceUrl());
            dataSource.setUsername(tenantInfo.getDatasourceUsername());
            dataSource.setPassword(tenantInfo.getDatasourcePassword());
            dataSource.setDataSourceProperties(master.getDataSourceProperties());
            dataSourceCachePool.put(tenantInfo.getTenantId(), dataSource);
        }
        //设置数据源
        tenantRoutingDataSource.setDataSources(dataSourceCachePool);
    }
}
</code></pre>
<h2 id="二-方案的隐藏缺点以及解决">二、方案的隐藏缺点以及解决</h2>
<h3 id="隐藏的缺陷">隐藏的缺陷</h3>
<p>相信有一定开发经验的读者应该能想到，上述方案最大的缺点就是性能问题，对MySQL有非常大的影响。因为一开始初始化非常多的连接池，就会占用连接资源，比如租户从100个扩展到了1000个以及更多，那么连接池数量就线性增长，如果一个连接池保持15个活跃连接的话，那么连接数就是15*1000，此时如果MySQL的maxconntion的数量非常小，那么MySQL侧就会抛出”too many connctions“错误，在应用层方面就是MySQL不可用了。<br>
没优化之前的架构：<br>
<img src="https://zhangyaoo.github.io/post-images/1603190851131.png" alt="" loading="lazy"></p>
<h3 id="解决">解决</h3>
<p>想保持数据库分离，又要考虑到MySQL性能问题，只能向连接池优化的方向去考虑，其实可以减少数量就可以了，这里实现方案就是一个数据库实例一个连接池，如下图所示：<br>
<img src="https://zhangyaoo.github.io/post-images/1603190889253.png" alt="" loading="lazy"><br>
具体实现就是将上述方案中的dataSourceCachePool的key改为 “IP+端口”，作为key。然后再数据源路由层，多一层映射（租户ID——&gt;数据库实例）就可以了。</p>
<h2 id="三-更优雅方案解决企业内部开发痛点">三、更优雅方案解决企业内部开发痛点</h2>
<h3 id="现状">现状</h3>
<p><strong>现状</strong>：企业内部项目组开发数据源路由，各个人员开发水平不一，各种路由方案实现不同，自己组内的开发的方案只能自己组内使用，并且实现复杂，耗人力物力。<br>
<strong>目标</strong>：项目组使用直接引入maven包，任何配置都不要配置（自定义的话需要自行在自己项目中配置属性），开箱即用。</p>
<h3 id="具体实现">具体实现</h3>
<p><strong>原理</strong>：直接采用springboot starter开发，将上述方案所有的逻辑和技术实现单独放入springboot starter工程中，采用外部配置的方式实现自定义配置。</p>
<p><strong>开发者实现</strong>：网上有许多springboot starter开发的流程和开发案例，笔者这里就只贴出关键的代码<br>
1、自动装配类：spring.factories中写入这个类DataSourceAutoConfigure，实现bean的自动装入，类里面主要是实现SqlSessionFactory和PlatformTransactionManager，然后在TenantRoutingDataSource的getconnection方法中自定义实现路由逻辑。</p>
<pre><code class="language-java">@Configuration
public class DataSourceAutoConfigure {

    @Resource
    private TenantRoutingDataSource tenantRoutingDataSource;

    @Bean
    @ConditionalOnMissingBean(SqlSessionFactory.class)
    @ConditionalOnBean(TenantRoutingDataSource.class)
    public SqlSessionFactory sqlSessionFactory() throws Exception{
        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();
        sqlSessionFactoryBean.setDataSource(tenantRoutingDataSource);
        Objects.requireNonNull(sqlSessionFactoryBean.getObject()).getConfiguration().setMapUnderscoreToCamelCase(true);
        return sqlSessionFactoryBean.getObject();
    }

    @Bean
    @ConditionalOnMissingBean(PlatformTransactionManager.class)
    @ConditionalOnBean(TenantRoutingDataSource.class)
    public PlatformTransactionManager platformTransactionManager() {
        return new DataSourceTransactionManager(tenantRoutingDataSource);
    }
}
</code></pre>
<p>2、dubbo filter扩展接口：获取租户ID，并且需要加@Activate注解，这样dubbo在初始化filter链的时候，自动将这个filter注册到filter链中，这样做的好处就是，用户在自己工程中不需要配置filter这个参数，无需增加任何的配置。</p>
<pre><code class="language-java">@Activate(group = {&quot;provider&quot;})
public class TenantCodeContextFilter implements Filter {
    @Override
    public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException {
        String tenantCode = RpcContext.getContext().getAttachment(&quot;tenantCode&quot;);
        TenantCodeContextHolder.setTenantCode(tenantCode);
        return invoker.invoke(invocation);
    }
}
</code></pre>
<p>3、检查用户侧自定义配置是否正确：检查用户的配置是否合理，不合理的话再容器就绪阶段就会抛出异常</p>
<pre><code class="language-java">@Component
public class CheckConfigListener implements ApplicationListener&lt;ApplicationReadyEvent&gt; {

    @Override
    public void onApplicationEvent(ApplicationReadyEvent applicationReadyEvent) {
        ConfigurableApplicationContext applicationContext = applicationReadyEvent.getApplicationContext();
        ConfigurableEnvironment environment = applicationContext.getEnvironment();
        // 检查用户自定义配置是否正确，自行实现
        checkDatasourceConfig(environment);
    }
}
</code></pre>
<p><strong>用户使用</strong>：直接引入相应的maven，方便快捷</p>
<h2 id="四-todo后续优化">四、TODO后续优化</h2>
<ol>
<li>目前多租户数据源通用工程只支持Dubbo的调用，未来可扩展支持多种协议如HTTP、gRPC</li>
<li>目前只支持Hikari数据源，后续支持多种数据源类型，比如Durid</li>
<li>如果租户数据非常大，可以考虑空间换时间思想，使用缓存存放租户的数据源配置，提升查询效率。</li>
</ol>
<h2 id="参考">参考</h2>
<ul>
<li>SaaS系统数据隔离方案——https://blog.arkency.com/comparison-of-approaches-to-multitenancy-in-rails-apps/</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MySQL联合索引在B+树的存储和查找]]></title>
        <id>https://zhangyaoo.github.io/post/mysql-lian-he-suo-yin-zai-bshu-de-cun-chu-he-cha-zhao/</id>
        <link href="https://zhangyaoo.github.io/post/mysql-lian-he-suo-yin-zai-bshu-de-cun-chu-he-cha-zhao/">
        </link>
        <updated>2020-06-29T09:32:43.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<blockquote>
<p>在对MySQL开发中，联合索引是很常见的一种MySQL优化方式，本文解释了联合索引的存储以及查找过程，可以了解一下底层的原理以及加深对MySQL联合索引的理解。</p>
</blockquote>
<h2 id="innodb-b树">Innodb B+树</h2>
<p>先看一下Innodb B+树的主键索引和辅助索引。这里直接拿张洋大神的图：</p>
<ul>
<li>聚簇索引:<br>
<img src="https://zhangyaoo.github.io/post-images/1593425796639.png" alt="" loading="lazy"></li>
<li>辅助非聚簇索引:<br>
<img src="https://zhangyaoo.github.io/post-images/1593425801180.png" alt="" loading="lazy"><br>
<strong>结构</strong>：当一个表T（id,name,age,sex,high）建一个普通索引  KEY(name)，name的索引结果就和上面辅助非聚簇索引结构一样。<br>
<strong>查询</strong>：当有一个select id,name,age from T where name = &quot;&quot; 辅助索引会根据name在B+树上进行二叉树查找，找出叶子节点数据后发现没有age这个数据，就会进行<strong>回表</strong>操作到主键聚簇索引去查找，拿到聚簇索引叶子节点的age数据。</li>
</ul>
<h2 id="联合索引存储以及寻址">联合索引存储以及寻址</h2>
<ul>
<li>
<p><strong>索引结构</strong>：我们知道上述回表过程也会消耗性能，相当于多查一次，所以系统可以根据业务情况加上一个组合索引，当然并不是一直加组合索引就可以了，因为要考虑到索引存储空间的问题。例如给上述加上一个组合索引  KEY（name,age,sex）【 KEY（col1,col2,col3）】。那么这个组合索引的B+树非叶子节点数据结构和上述辅助非聚簇索引图一样，但是叶子节点是这样的：<br>
<img src="https://zhangyaoo.github.io/post-images/1593425790647.png" alt="" loading="lazy"><br>
叶子节点存储col1,col2,col3这三列数据以及加上ID这一列数据。</p>
</li>
<li>
<p><strong>寻址过程：</strong><br>
例如语句：select id,name,age from T where name = &quot;张三&quot; and age=25，先根据name字段从辅助聚簇索引定位到哪一个叶子节点数据中，然后根据age节点在上述表格的前6行中，寻找age= 25的数据，然后找出所有符合的数据以及其对应的ID，然后根据ID来进行回表操作查询。这里返回了三条数据，就回了三次表。<br>
上述回表过程中，笔者引入一个<strong>索引下推</strong>的一个功能，索引下推是MySQL在5.6版本后引入的一个查询优化。就拿上述的例子，在没有优化之前，据name字段查询“张三”后，会拿到6条结果，回表6次，然后从主键索引拿到6条数据后，根据age字段筛选数据；优化之后，先再辅助索引上面根据name字段和age字段筛选符合数据，也就是ID，然后再回表，这里回表了三次。</p>
</li>
<li>
<p><strong>组合索引注意事项</strong><br>
当然，联合索引的最重要的是注意联合索引的使用问题，要遵循最左匹配原则，才可以优化到整个SQL了。</p>
</li>
</ul>
<h3 id="总结">总结</h3>
<p>以上，总结了MySQL的索引的基本原理，以及联合索引的存储和寻址过程，并且引入索引下推概念，还有使用联合索引的注意事项。</p>
<h2 id="参考">参考</h2>
<ul>
<li>MySQL索引背后的数据结构及算法原理——http://blog.codinglabs.org/articles/theory-of-mysql-index.html。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[关于Zookeeper奇数节点以及脑裂问题]]></title>
        <id>https://zhangyaoo.github.io/post/guan-yu-zookeeper-qi-shu-jie-dian-yi-ji-nao-lie-wen-ti/</id>
        <link href="https://zhangyaoo.github.io/post/guan-yu-zookeeper-qi-shu-jie-dian-yi-ji-nao-lie-wen-ti/">
        </link>
        <updated>2020-06-22T02:32:00.000Z</updated>
        <content type="html"><![CDATA[<h3 id="前言">前言</h3>
<blockquote>
<p>Zookeeper作为微服务分布式协调中间件，了解它的原理以及日常开发中的注意事项和可能会出现的问题是有必要的。</p>
</blockquote>
<h3 id="前置知识zab协议">前置知识：ZAB协议</h3>
<p>ZAB：Zookeeper Atomic Broadcast（ZAB）崩坏恢复和原子广播协议<br>
1）崩坏恢复：在master节点宕机情况下，其他集群节点会重新选举master节点，快速领导者选举机制：选举规则会参照最大的分代年龄epoch&gt;最大的事务zxid&gt;server id来进行选举，选举过程就是将自己节点投票信息发给其他集群节点，投票信息附带zxid和serverid，<strong>判断是否超过一半的投票选同一个节点</strong>，那么这个节点就会选举为master。<br>
2）选举完后，就会进行数据同步，将master节点数据同步到slave中，此时对外服务不可用。<br>
3）原子广播：ZAB协议保证消息的一致性和有序性<br>
 一致性：leader发送propasal事务请求（包含zxid），master判断过半机制ack，就认为事务可以提交了，master会提交事务，然后广播提交事务消息，从节点开始提交本事务。一半ack机制，可以看zookeeper是CP，但是不是强一致性；从节点接收propasal后，会将事务写入磁盘。<br>
 有序性：zxid事务id保证全局有序性，每一个slave服务器维持一个FIFO队列，维持局部有序性。</p>
<h3 id="zookeeper脑裂">Zookeeper脑裂</h3>
<p> Zookeeper脑裂都是出现在集群环境中的。指的是一个集群环境中出现了多个master节点，导致严重数据同步和写入问题，数据不一致等等，如果这种情况出现在线上分布式环境下，会导致服务不可用。</p>
<h3 id="出现原因">出现原因</h3>
<p> 可能就是网络环境有问题导致节点之间断开，或者节点假死等等，导致一部分slave节点会重新进入崩坏恢复模式，重新选举新的master节点，然后对外提供事务服务。由于心跳超时（网络原因导致的）认为旧的master死了，但其实旧的master还存活着。</p>
<h3 id="如何解决脑裂">如何解决脑裂</h3>
<p>过半机制，如果集群中某个节点的投票数量大于集群有效节点的一半，就会选出master。这里拿出关键代码：</p>
<pre><code class="language-java">// 验证是否符合过半机制，如果符合就会选举新的master节点
public boolean containsQuorum(Set&lt;Long&gt; set){
    // half是在构造方法里赋值的
    // n表示集群中zkServer的个数（准确的说是参与者的个数，参与者不包括观察者节点）
    half = n/2;
    // set.size()表示某台zkServer获得的票数
    return (set.size() &gt; half);
}
</code></pre>
<p>笔者介绍几种情况，来说明一下几种脑裂的场景</p>
<ul>
<li>
<p>比如集群中有6个节点，一个master和5个slave，分两个机房，每个机房分别三台，发生了机房不可通信的情况，如下图：<br>
<img src="https://zhangyaoo.github.io/post-images/1593419532309.png" alt="" loading="lazy"><br>
然后机房B就会产生新的master，如图<br>
<img src="https://zhangyaoo.github.io/post-images/1593419550068.png" alt="" loading="lazy"><br>
这个时候Zookeeper为了防止这样的情况发生，利用了<strong>过半机制</strong>的这个特性。<br>
上图中，机房B节点为3 小于集群数量的一半，所以，最终上面图中机房B是不会选举出新的master节点的。</p>
</li>
<li>
<p>我们再来看一种情况：比如集群中有5个节点，一个master和4个slave，分两个机房，如下图：<br>
<img src="https://zhangyaoo.github.io/post-images/1593419973725.png" alt="" loading="lazy"><br>
如果发生了机房不能通信的情况，那么机房B因为节点是2个，没有超过一半，就不会产生出新的master节点了。</p>
</li>
<li>
<p>再来看最后一种情况，比如集群中有5个节点，一个master和4个slave，分两个机房，不同的是master节点在机房B，如下图：<br>
<img src="https://zhangyaoo.github.io/post-images/1593420189914.png" alt="" loading="lazy"><br>
如果发生了机房不能通信的情况，那么机房A节点是3个，超过了一半，就会进入崩坏恢复模式产生新的master节点，那么此时集群中就会出现两个master节点了。如下图所示<br>
<img src="https://zhangyaoo.github.io/post-images/1593420219104.png" alt="" loading="lazy"><br>
那么遇到这种情况Zookeeper是如何处理的？答：旧的leader所有的写请求同步到其他followers节点是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。这个时候旧的master进入恢复模式进行数据同步。<br>
所以按照上面的情况，机房A的所有followers节点正常通信，机房B的所有节点重新进入恢复模式进行数据同步。</p>
</li>
</ul>
<p>总结：通过Quorums机制来防止脑裂，当leader挂掉之后，可以重新选举出新的leader节点使整个集群达成一致；当出现假死现象时，通过epoch大小来拒绝旧的leader发起的请求，当出现这种情况，旧的leader 进入恢复模式进行数据同步。</p>
<h3 id="引出奇数节点">引出奇数节点</h3>
<p> 知晓以上场景后，我们知道，2台机器也能选举出master，只不过只要有1个死了zookeeper就不能用了，因为1没有过半。所以2个zookeeper的死亡容忍度为0。同理，要是有3个zookeeper，一个死了，还剩下2个正常的，过半了，所以3个zookeeper的容忍度为1。如果按照这样的机制推理，那么得出2-&gt;0;3-&gt;1;4-&gt;1;5-&gt;2;6-&gt;2  左边是数量，右边是容忍度，所以2n和2n-1的容忍度是一样的，所以可以得出，集群是<strong>奇数个能够节省资源</strong>。</p>
<!--下面的奇数节点的作用需要确认 TODO-->
<h3 id="总结">总结</h3>
<p>以上，笔者总结了ZAB协议，到Zookeeper防止脑裂的场景以及如何处理，以及结合例子，Zookeeper集群在奇数节点下的作用。</p>
<h3 id="参考">参考</h3>
<ul>
<li>ZooKeeper集群的脑裂问题——https://www.cnblogs.com/shoufeng/p/10591526.html</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[金融级业务下分布式事务保证数据一致性]]></title>
        <id>https://zhangyaoo.github.io/post/jin-rong-ji-ye-wu-xia-fen-bu-shi-shi-wu-bao-zheng-shu-ju-yi-zhi-xing/</id>
        <link href="https://zhangyaoo.github.io/post/jin-rong-ji-ye-wu-xia-fen-bu-shi-shi-wu-bao-zheng-shu-ju-yi-zhi-xing/">
        </link>
        <updated>2020-06-22T02:27:01.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<blockquote>
<p>随着分布式服务架构的流行与普及，原来在单体应用中执行的多个逻辑操作，现在被拆分成了多个服务之间的远程调用。微服务化后，随着带来的服务之间的分布式事务问题，尤其是在金融业务下，分布式事务是保证数据一致性的重要保证。本文着重会讲分布式事务场景和业界主流的解决方案。</p>
</blockquote>
<h2 id="一-引入">一、引入</h2>
<p> 资金转账在金融业务下是一个非常重要而且常见的场景，如果因为技术问题导致资金转账错误，导致数据不一致问题，那么就会造成无法预测的后果。<br>
 笔者这里拿银行转账的例子来说（这里的转账有很多场景比如银行卡之间充值提现、银行账户之间的转账等等），比如甲银行账户A向乙银行账户B转账1W：</p>
<p>同步调用：</p>
<ol>
<li>A银行对转出账户执行检查校验，进行账户金额扣减。</li>
<li>A银行同步调用B银行转账接口。</li>
<li>B银行对转入账户进行检查校验，进行账户金额增加。</li>
<li>B银行返回处理结果给A银行。<br>
<img src="https://zhangyaoo.github.io/post-images/1593745125729.png" alt="" loading="lazy"><br>
同步调用问题：</li>
</ol>
<ul>
<li>如果B银行因为网络原因导致接口不通，那么A调用线程会长时间阻塞。</li>
<li>如果A扣减后，发送请求后，在网络中丢失了，B银行没有收到请求，导致账户A扣减了，账户B没有加</li>
<li>如果账户B扣减成功了，由于某种原因比如网络异常没有及时回调给甲银行，那么账户A就认为是异常请求，则会回滚事务，导致数据不一致。</li>
</ul>
<p>再来看一下异步调用：</p>
<ol>
<li>A银行对转出账户执行检查校验，进行账户金额扣减。</li>
<li>主线程将请求数据异步写入队列MQ</li>
<li>真正消费者程序对B银行进行远程调用</li>
<li>B银行对转入账户进行检查校验，进行账户金额增加。</li>
<li>B银行返回处理结果给A银行。<br>
<img src="https://zhangyaoo.github.io/post-images/1593743096206.png" alt="" loading="lazy"><br>
异步调用问题：</li>
</ol>
<ul>
<li>如果账户A扣减本地事务成功了，但是消息发出后，因为网络原因或者其他宕机原因，导致消息未发送成功，没有进行B账户远程调用，导致本地事务和消息不一致性。</li>
<li>MQ消费端程序如果消费消息成功，请求银行成功了，但是回传ACK给MQ失败了，那么回导致消费端程序重复消费问题，那么就会出现重复转账的问题。</li>
<li>在B账户因为某些原因导致账户增加失败，在回调A银行接口通知回滚时网络异常或者宕机，会导致A银行转账无法完成回滚，从而导致数据不一致。</li>
</ul>
<p>异步调用解决了同步调用的主线程阻塞问题，但还是没有解决数据一致性问题。而且引入MQ中间后，还要考虑到本地事务和MQ消息一致性问题，还有其他的引入后的维护工作，比如消息丢失，消息重发等等问题。</p>
<h2 id="二-分布式事务解决方案">二、分布式事务解决方案</h2>
<p> 讲到了分布式事务，自然离不开分布式系统的一些基本原则和定理：CAP原则和BASE理论，相信读者应该都知道，这里不做过多阐述。业界根据这些规则和理论，衍生出了各种分布式事务解决方案：XA规范，2PC，3PC，本地消息表方案，基于消息中间件的最终一致性方案，TCC方案，阿里的SEATA，SAGA方案和最大努力通知等等。<br>
 以上每个方案都有自己的应用场景，就拿2PC来说，MySQL的事务型日志redolog二段提交（redolog(prepare)--》binlog--》redolog(commit)）保证binlog和redolog数据一致性，Zookeeper的proposal事务二段提交（半数以上ack返回成功表示写入数据成功）保证leader和foller的数据一致性，这些都是2PC的应用。<br>
 金融场景下类似资金业务需要保证最终一致性解决分布式事务，不需要保证转账实时性。所以本地消息表、基于MQ中间件的最终一致性等柔性方案是首选的方案。这些基于消息的分布式事务，本质上就是，本地事务+从事务，从事务从消息中获取信息进行本地提交，这里保持<strong>异步事务机制、只能保证最终一致性</strong>。</p>
<h3 id="21-利用本地消息表思想解决一致性问题">2.1 利用本地消息表思想解决一致性问题</h3>
<p> 一般来说，跨行转账的原理，会存在一个中国人民银行的中间人角色来操作转账，但不在本次讨论的范围内。<br>
 业界银行转账大部分都是同步转账，异步获取转账结果，包括第三方支付平台对接银行都是这样玩的。这里笔者就利用本地消息表思想来具体叙述数据一致性是如何保证的，老规矩先放图：<br>
<img src="https://zhangyaoo.github.io/post-images/1603274563955.png" alt="" loading="lazy"><br>
其中交易记录表大概长这个样子：</p>
<table>
<thead>
<tr>
<th style="text-align:center">字段</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">id</td>
<td style="text-align:center">自增ID，没有业务意义</td>
</tr>
<tr>
<td style="text-align:center">trade_order_num</td>
<td style="text-align:center">交易订单号，作为转账记录唯一标识</td>
</tr>
<tr>
<td style="text-align:center">source_account_num</td>
<td style="text-align:center">交易转出方账户ID</td>
</tr>
<tr>
<td style="text-align:center">target_account_num</td>
<td style="text-align:center">交易收款方账户ID</td>
</tr>
<tr>
<td style="text-align:center">status</td>
<td style="text-align:center">状态机，0=预创建，1=转账中，2=转账成功，3=转账失败</td>
</tr>
<tr>
<td style="text-align:center">pay_success_time</td>
<td style="text-align:center">记录转账成功时间</td>
</tr>
<tr>
<td style="text-align:center">create_time</td>
<td style="text-align:center">记录创建时间，可作为窗口时间内判断标准</td>
</tr>
<tr>
<td style="text-align:center">update_time</td>
<td style="text-align:center">记录 更新时间</td>
</tr>
</tbody>
</table>
<p>账户表大概长这个样子：</p>
<table>
<thead>
<tr>
<th style="text-align:center">字段</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">id</td>
<td style="text-align:center">自增ID，没有业务意义</td>
</tr>
<tr>
<td style="text-align:center">account_num</td>
<td style="text-align:center">账户ID</td>
</tr>
<tr>
<td style="text-align:center">current_amt</td>
<td style="text-align:center">当前账户余额</td>
</tr>
<tr>
<td style="text-align:center">lock_amt</td>
<td style="text-align:center">冻结金额，用来记录临时状态的核心转账数据 。真实余额=current_amt-lock_amt</td>
</tr>
</tbody>
</table>
<p>图中的步骤大致分为10步，分别是：<br>
1.</p>
<h3 id="22-事务消息解决本地事务和mq消息一致性问题">2.2 事务消息解决本地事务和MQ消息一致性问题</h3>
<h4 id="221-消费端重复消费">2.2.1 消费端重复消费</h4>
<h4 id="222-消费端消费失败">2.2.2 消费端消费失败</h4>
<h3 id="23-其他问题">2.3 其他问题</h3>
<h4 id="231-银行交易冲正">2.3.1 银行交易冲正</h4>
<h2 id="三-其他方式保证数据一致性">三、 其他方式保证数据一致性</h2>
<p> 当然，保持数据一致性不光是分布式事务来保证，业务上还要配合其他的辅助来保证，这里笔者就列举几种</p>
<ol>
<li>全链路幂等<br>
全链路幂等保证不产生脏数据，保护核心流程正常执行。</li>
<li>业务对账<br>
业务内部准实时对账，比如业务发生后充值提现，对比用户余额是否正确，用户业务流水是否正确。<br>
T+1日对账，程序或者人工定时扫描核心业务数据，保证当日数据准确。对账后自动检测并且修复重试业务</li>
<li>业务指标监控<br>
监控数据库中的订单预占资金没有释放，状态机是不是最终态监控，单位窗口时间内业务状态是否异常，账户中的预扣减金额是否释放，业务重试次数是否超过阈值等等业务监控。</li>
</ol>
<h2 id="四-总结">四、总结</h2>
<p> 分布式场景，要用分布式的思维去思考问题。要考虑任何的超时，断电，维护不同物理存储的数据的可能存在的状态不一致的场景，说白了要面向失败编程。</p>
<h2 id="五-参考">五、参考</h2>
<ul>
<li>有赞出金系统——https://tech.youzan.com/build-a-withdraw-sys/</li>
<li>分布式事务的思考——https://www.cnblogs.com/sujing/p/11006424.html</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[O(N)时间复杂度下，二进制反转]]></title>
        <id>https://zhangyaoo.github.io/post/er-jin-zhi-fan-zhuan/</id>
        <link href="https://zhangyaoo.github.io/post/er-jin-zhi-fan-zhuan/">
        </link>
        <updated>2020-06-19T08:11:36.000Z</updated>
        <content type="html"><![CDATA[<h4 id="题目描述">题目描述</h4>
<p>给定一个32位整数 . 输出二进制表示反转后的值.<br>
例如 input 43261596（二进制 00000010100101000001111010011100）<br>
返回 output 964176192（二进制 00111001011110000010100101000000）</p>
<p>目前笔者就想到了时间复杂度在O(N)的解决思路：</p>
<ol>
<li>循环判断输入数据的低位是0还是1，具体判断方法是和1进行与操作</li>
<li>如果判断是，返回的结果+1，不是1那么不做任何处理</li>
<li>每次循环，input的数据向左移一位，output数据向右移动一位</li>
<li>循环32次，返回结果</li>
</ol>
<pre><code class="language-java">/**
    *  二进制数据反转
    */
public class BitReverse {

    public static int reverse(int n) {
        int result = 0;
        for (int i = 0; i &lt; 32; i++) {
            result = result &lt;&lt; 1;
            if ((n &amp; 1) == 1) {
                result++;
            }
            n = n &gt;&gt; 1;
        }
        return result;
    }

    public static void main(String[] args){
        System.out.println(reverse(1&lt;&lt;30));
        System.out.println(1&lt;&lt;30);
    }
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[系统性能优化的思考和总结]]></title>
        <id>https://zhangyaoo.github.io/post/guan-yu-xi-tong-xing-neng-ping-jing-yu-ce-he-xi-tong-xing-neng-you-hua/</id>
        <link href="https://zhangyaoo.github.io/post/guan-yu-xi-tong-xing-neng-ping-jing-yu-ce-he-xi-tong-xing-neng-you-hua/">
        </link>
        <updated>2020-06-17T13:05:11.000Z</updated>
        <content type="html"><![CDATA[<h3 id="前言">前言</h3>
<blockquote>
<p>对于任何系统，都会存在系统性能瓶颈，这里笔者作为一名Java工程师列出了自己在工作中的优化思路，仅供参考。</p>
</blockquote>
<h2 id="一-系统性能预测">一、系统性能预测</h2>
<p> 任何一个系统都是从0到1慢慢发展的，当系统业务随着时间的推移，业务量和流量随之增大，系统性能就随着凸显出来。这个时候，开发人员和架构师要从架构层面、代码层面、产品业务层面等一一去演进业务系统来维持高流量下系统稳定性。</p>
<p> 现在服务都是微服务部署开发，如果要模拟服务压测的话要在本地开启相同的服务，前提是机器配置是一样的。而且需要将线上的持久化数据同样copy到本地数据库中，这样才能真正模拟线上的环境。拿单台机器进行压测，压测的对象可以是某个核心的接口或者业务模块（这个接口可以是日志服务中统计的访问量比较高的具体的接口API），压测的指标可以是吞吐量，平均响应时间，最大响应时间，TPS，QPS等等。</p>
<p> 通过性能指标可以度量目前存在的性能问题，同时作为性能优化的评估依据。具体的指标主要是要分析系统的QPS、TPS、平均响应时间以及最大响应时间，我们预测一个单体的应用能够承受多少的并发量，看这这些指标是否能够达到我们预期的值，比如作为一个健康的系统，最大响应时间不超过1s。后面进行压测时候，观察流量巅峰时刻观察系统的运行情况。以下就性能分析优化展开总结。<br>
<br></p>
<h2 id="二-系统性能分析优化">二、系统性能分析优化</h2>
<h3 id="1-硬件方面">1、硬件方面：</h3>
<h5 id="cpu">CPU：</h5>
<p> 在压测的时候观察CPU的占用情况，是否长期处于100%状态，正常来说80%以下是正常的。如果非常低，那么说明系统不是在做IO密集型运算动作，性能瓶颈是在其他方面，不是在CPU上面，具体的操作方法可以用top命令查看。<br>
 以笔者经验来看，一般CPU飙高的原因无非三种：</p>
<ol>
<li>第一种就是代码中存在死循环，并且循环中有大量的CPU计算操作；</li>
<li>第二种就是多线程并发下，竞争相同的资源导致大量线程获取不到资源，如果此时线程进行自旋操作，不释放CPU资源，那就导致CPU飙升；</li>
<li>第三种就是代码中有内存泄漏，导致内存一直处于阈值状态，GC线程会持续GC，导致CPU飙高。</li>
</ol>
<p> 上面三种情况中第二种和第三种情况在日常开发工作中会遇到，对于第二种情况对于自旋锁情况，一般会用CAS乐观锁去实现，并且设置一定的超时时间和重试次数，然后返回失败或者进入阻塞队列释放CPU分片，防止线程一直占用CPU资源。<br>
 对于第三种情况，就是代码的BUG，开发过程中要注意泄漏的问题，就比如多线程操作链表，如果没有做同步的锁，那么很有可能导致链表的引用指针混乱，引起内存泄漏。<br>
 以上，如果我们开发工作中避免了上述几种情况，CPU就能够发挥它应该有的能力，提升系统性能。同时，开发人员做好硬件CPU监控是非常有必要的。</p>
<h5 id="内存">内存：</h5>
<p>  在Java中，内存JVM是一个很重要的指标，这个关乎到系统是否可以稳定运行。我们可以借助三方工具可以查看系统的JVM内存的运行情况，笔者提供几个通用的预测内存的运行情况的思路：<br>
- 每秒占用多少内存？<br>
- 多长时间触发一次Minor GC？<br>
- 多长时间触发一次Major GC？<br>
- Minor  GC耗时多久？Major  GC耗时多久？<br>
- 会不会频繁因为Survivor放不下导致对象进入老年代？</p>
<p>在日常开发中，开发人员需要关注的就是，判断系统JVM是否有频繁FULL GC和频繁YOUNG GC，如果有，那么会严重影响系统性能。笔者就这两个方面去分析一下<br>
   1、<strong>频繁FULL GC</strong> ：首先我们应该要了解到频繁FULL GC危害，一般的中大型系统，系统的JVM会设置很大，比如会给堆内存分配4~8G的空间，因为遍历对象图的过程中堆越大，遍历时间就会长，而且如果垃圾越多，垃圾回收也会拉长整个GC的时间，这就导致每一次FULL GC会有长时间的STW，影响系统稳定性。然后我们要清楚导致触发Full GC的场景，这里列出了可能会导致的几个场景：<br>
  1）大对象<br>
  2）方法区meta space空间占满<br>
  3）年轻代的存活的生命周期长对象一直汇入老年代，导致GC<br>
  4）内存泄漏导致空间不足进而GC<br>
这里笔者就拿内存泄漏（内存泄漏指的是有引用无法被回收但是没有用的对象持续增长）来说，一般如果有内存泄漏。大概的内存监控图长这个样子<img src="https://zhangyaoo.github.io/post-images/1592463486063.png" alt="" loading="lazy"><br>
这样导致的后果就是，频繁的FULL GC，最后内存一直持续增长到爆满，然后FULL GC执行间隔缩短，最终会导致GC线程持续GC，CPU使用率会直线飙升，导致系统瘫痪。<br>
    2、 <strong>频繁 YOUNG GC</strong>  ：YOUNG GC如果过于频繁的话，一般是短周期小对象较多，这时候可以从 Eden 区/新生代设置的太小了这个方面考虑，看能否通过调整-Xmn、-XX:SurvivorRatio 等参数设置来解决问题</p>
<p><strong>这里笔者以自己开发经验，提供一些“简单的”JVM优化拙见</strong>：</p>
<ol>
<li>尽量将新生代的垃圾回收掉，不让存活对象进入老年代，因为老年代的GC代价比年轻代高，这里可以设置分代年龄-XX:MaxTenuringThreshold=XX<br>
  例子1：比如说业务上一分钟产生几百兆的数据，而且需要存活一分钟，如果一分钟YGC的次数少于默认分代年龄，那么对象会进去老年代引发FGC，FGC会引起更大的停顿时间<br>
  例子2：如果说对象都是一些短期对象，那么可以设置分代年龄更小，因为长期对象肯定是大对象或者单例对象永驻内存的，这样可以腾出空间给新生代GC，避免新生代频繁GC</li>
<li>增加新生代内存的大小，防止导致频繁的minor GC，这样老年代的Major GC频率也会降低</li>
<li>尽量将大内存的服务，拆分成几个相同服务，也就是多实例部署，分散堆内存资源，避免堆大内存导致GC时间过长（这个和G1分区回收思想相似）</li>
<li>每个线程占用的内存不应过大或者过小，不然会导致OOM<br>
  如果线程内存过小，会导致线程里面的栈内存小，临时变量如果超出这个阈值就会无法分配栈，导致栈溢出，出现stackoverflow<br>
  如果线程内存过大，在多线程并发下，如果线程数量过多，会占用非常多JVM内存，有内存溢出的风险</li>
<li>合理设置垃圾回收器，在大内存或者在内存碎片化环境下，G1垃圾回收器会有很好的效果<br>
  G1垃圾回收器是Java9默认回收器，G1能够在指定的停顿时间内，根据每个region的回收价值，选择可以去回收的region，并且存活对象移动复制是多线程进行的。这里要注意如果设置停顿时间的话，不能设置太小，因为太小会导致每次进行回收的region太少，导致垃圾回收速度更不上垃圾生产的速度，这样随着时间推移，系统垃圾对象会越来越多，占满JVM</li>
<li>对象生命周期的分布情况：如果应用存在大量的短期对象，应该适当增大年轻代 -Xmn；如果存在相对较多的持久对象，老年代应该适当增大。-Xms -Xmx</li>
<li>Xms和Xmx也设置为相同，这样可以减少内存自动扩容和收缩带来的性能损失</li>
<li>设置大对象对象的大小，一般系统中大对象大部分都是一些系统的缓存，像这些对象尽早让它们的进入老年代，避免占用新生代的空间。</li>
</ol>
<p>以上，合理分配JVM内存资源以及做好系统内存的监控机制是我们系统稳定性运行的保障。</p>
<h5 id="网络负载和io">网络负载和IO</h5>
<p>来一张IO发生场景图片：<br>
<img src="https://zhangyaoo.github.io/post-images/1593333928024.png" alt="" loading="lazy"><br>
 对于磁盘IO，我们可以用Linux下的iostat命令去查看当前IO负载的情况，比如r_wait和w_wait指标，这些指标较大则说明I/O负载较大，I/O等待比较严重，磁盘读写遇到瓶颈。这个时候我们要看压测的接口是否有文件读取和写入的操作，如果有说明接口性能瓶颈在于文件读写，这个时候可以利用文件buffer缓存API等功能进行优化，或者可以用异步的方式进行文件读写。<br>
 笔者在开发中就遇到因为IO问题带来的线程资源耗尽的线上问题：我们系统业务在借贷业务成功后，要生成借款协议，协议是一个PDF文件，当时主业务逻辑完成后同步调用生成PDF的逻辑，因为当时大流量并发，导致整个借贷业务性能瓶颈就在磁盘IO上，CPU处于空闲状态，借用网上的一个图，TOP命令可以看出IO花费的时间在76.6%，后面优化后就多线程异步处理。<br>
<img src="https://zhangyaoo.github.io/post-images/1593334249512.webp" alt="" loading="lazy"></p>
<p> 对于网络负载，因为网络负载或者网络堵塞是不受控制的，这个涉及到底层的TCP通信的优化（比如利用滑动窗口和拥塞控制），这个就不展开讨论。工程师可控范围可以是选择IO读写高效率的中间件，比如redis、tomcat、activeMq、nginx、dubbo、netty等，这些中间件的底层IO模型的是多路复用IO，多路复用IO指的是一个IO线程能够服务于多个socket连接，线程监听每个连接的资源描述符。如下图所示：<img src="https://zhangyaoo.github.io/post-images/1593334346151.png" alt="" loading="lazy"></p>
<p> 分布式微服务环境下，服务之间的RPC同步调用会非常频繁，随之服务之间的网络负载会影响到整个系统的服务性能，因此，每个服务的机器放置到同一个局域网下性能效果会很好。而且，对于服务之间的调用，最好利用自研或者第三方中间件去监控服务链路调用的整体情况（比如Zipkin或者SkyWalking ）,并且要合理设置服务与服务之间的超时时间，避免因为网络原因导致服务线程池耗尽，导致OOM。</p>
<h3 id="2-中间件层">2、中间件层</h3>
<p>  这里中间件，泛指数据存储层，以笔者经验来看，大多数系统性能问题和瓶颈都是与数据存储相关，这里笔者就拿这方面展开讨论总结。</p>
<h5 id="mysql">MySQL</h5>
<p>一般来说MySQL在很多线程更新同一行的场景下，TPS性能曲线如图所示，参考丁奇的《秒杀场景下MySQL的低效》<br>
<img src="https://zhangyaoo.github.io/post-images/1592893242022.png" alt="秒杀场景下MySQL的低效" loading="lazy"><br>
图中我们可以看到，线程数在6的时候TPS达到巅峰2W，随着线程数的增长，TPS会随之降低。在高并发场景下，可以根据这个结论来进行优化，比如，当有瞬间大流量冲击数据库时候，我们可以进行数据缓冲，比如用队列削峰，开启6个线程消费，然后访问数据库。<br>
当然这个看业务场景，如果是对同一个资源进行竞争的话，这个证削峰是可行的。但是，如果场景是每一个线程对不同资源进行访问修改时候，不涉及资源竞争的话，那么就不要进行削峰处理，直接访问数据库即可，当然这个也要考虑到MySQL的性能问题。<br>
举个例子，就拿光插入数据的性能测试来说（没有建唯一索引），4核4G的7200转的机械硬盘机器配置，最高能够承受7500的并发插入数。[参考MySQL性能压测]</p>
<p>以上算是一种在特定场景下的优化的思路，下面笔者讨论一下日常开发中通用的MySQL优化：<br>
1、避免长期的事务锁占用，避免锁范围过大，避免单个资源的并发竞争</p>
<ul>
<li>首先我们知道数据库Innodb存储引擎的RR和RC隔离级别下，类似update语句，锁的释放时机是在事务提交之后，这个叫做两阶段锁协议。所以为了避免事务之间锁同一行数据出现长时间的互相等待的场景，<strong>要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放</strong>。<br>
举个例子，个人账户A转账给公共账户B，流程是：开启事务——》给 B 的账户余额增加钱；从账户 A 账户余额中扣除钱；记录一条交易日志——》结束事务。因为公共账户B可能被多个线程修改，所以可以优化为：从账户 A 账户余额中扣除钱；记录一条交易日志；给 B 的账户余额增加钱。</li>
<li>然后，要尽量将锁细化，一个大锁可以分割为多个锁，类似<strong>分段锁机制</strong>。拿笔者公司业务来说，比如APP上投资某一个产品标（包含了标的开始募集时间、结束时间、可投金额、年利率等等），在到达开始募集时间会有一段时间的高并发投标，这个时候会对具体标的行数据进行频繁的更新操作，就是扣减剩余可投金额，如果其他耗时操作中有对同一资源进行竞争的话，那么产品锁持有时间过长，导致性能低。如果有高并发秒杀下单等动作，会造成行锁抢占问题。<br>
这个时候，优化思路是，将这个产品标在数据库分为10份，每一份的可投金额减少10倍，每一个投标请求进行随机路由分配到这10个小的产品标中。这样就减少锁的并发竞争问题，优化性能。<br>
但是笔者因为遇到这种分段锁的问题导致的<strong>死锁</strong>问题，场景是这样的，当一份投标的金额大于其中一份产品金额的话，会持有这份产品的锁，并且循环获取下一份小产品，这个时候如果有两个线程都大于小产品金额的话，有概率会产生死锁问题，笔者最终通过顺序加锁以及加上锁的过期时间解决了这个问题。</li>
<li>最后，对同一个资源的并发竞争，举个例子，像12306抢票、商城活动秒杀等都是对同一个有限资源进行竞争的场景，笔者认为，这种场景是非常难处理的，需要考虑到锁同步数据安全、并发竞争性能瓶颈、超卖等问题，都是会影响C端用户实际的体验的。<br>
像这种场景，优化的思路就是——&gt;<strong>能用分段锁的就不要用悲观锁，能用乐观锁的就不要用悲观锁，能用无锁编程的就不要用锁，能用异步的场景就不要用同步的场景，能在内存操作的就不要再放到数据库磁盘层面操作</strong>。当然，有些对于数据安全性要求很高的场景，比如金融，加锁是必要的。这个就是业务一致性和并发的折中考虑，这个需要考虑具体的业务场景。</li>
</ul>
<p>2、关闭死锁检测<br>
MySQL默认开启死锁检测，概念：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。死锁检测对数据库有非常大的性能影响，会消耗CPU资源，最后会压垮数据库。<br>
在这种并发场景下可以关闭死锁检测功能，会有明显的性能提升。当然关闭死锁检测也会带来问题，比如当死锁发生时，会一直持有锁资源，直至到超时时间后，释放，这段等待的时候可能会造成线程持续等待造成严重后果。所以为了避免死锁的发生，对行资源进行加锁的时候可以根据ID主键等进行<strong>顺序加锁</strong>。</p>
<p>3、SQL优化</p>
<ul>
<li>SQL避免多表连接查询、in和exits合理应用、考虑索引失效场景</li>
<li>在经常查询和排序的列上加索引，对离散度不高的不建议加锁，遵循索引规范</li>
<li>在写场景多余读场景的索引选择，唯一索引和普通索引的选择</li>
<li>尽量进行覆盖索引，避免回表查询</li>
<li>尽量建立联合索引，来进行索引复用</li>
<li>字符串的前缀索引的建立</li>
<li>用explain分析整个SQL的执行情况，包括执行计划、索引分析</li>
<li>修改数据比较多的字段场景尽量加索引，尽量使用行锁，避免表锁</li>
</ul>
<p>4、在大数据量的情况下（一般单表超2000W）的优化思路：</p>
<ul>
<li>加缓存，对于高并发读场景用缓存，一级缓存Redis或者二级缓存Cache</li>
<li>架构层面MySQL就做主从复制或主主复制，读写分离，可以在应用层做，效率高</li>
<li>垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统；</li>
<li>水平切分，因为水平切分会增加代码开发复杂度，所以能尽量避免就不要做。针对数据量大的表，对于日志流水、配置型等类型数据，进行归档操作；对于状态业务数据进行分库分表，这里就不展开讨论。其次，要选择一个合理的sharding key，为了有好的查询效率，表结构也要改动，做一定的冗余；</li>
</ul>
<h5 id="redis">Redis</h5>
<p>Redis作为开发人员接触最频繁的中间件，首先，笔者先拿出官网给出的Redis性能测试结果：从下图可以得出结论：redis单机测试结果是TPS是7W,QPS只能比这个数据更高。<br>
<img src="https://zhangyaoo.github.io/post-images/1592983835284.png" alt="" loading="lazy"><br>
当使用了管道pipline后性能大约提升了6倍，如下图所示<br>
<img src="https://zhangyaoo.github.io/post-images/1592986207480.png" alt="" loading="lazy"></p>
<p>根据官网的文档，影响Redis性能有以下因素，笔者认为开发人员做一些基准测试压测以及日常开发中使用Redis的时候，注意这些优化点，就能高效使用Redis：<br>
1、网络延迟和网络带宽，作为运维人员，最好将服务器和Redis服务部署到同一个局域网内，降低网络延迟。作为开发人员，尽量不要使用大key和大value，因为随着这种数据越来越多，在网络传输的时候会占用大部分网络宽带，举个例子，如果一个redis对象大小超过1KB，当你的QPS达到100万，会把你的千兆路由器的带宽打满，因此网络带宽可能就会成为性能瓶颈。<br>
2、使用pipline，当使用以太网访问Redis时，保据大小保持在以太网数据包大小（约1500字节）以下时，使用流水线进行聚合的命令特别有效。<br>
3、延时删除，当某一个redisObject很大的时候，做删除操作会长时间占有线程持有时间，影响性能，redis新版本有延迟删除的功能。<br>
4、使用scan代替keys，keys会造成严重的性能问题<br>
5、设置内存的大小阈值并且设置好内存缓存淘汰的策略，线上设置LRU策略来淘汰缓存这样做是为了避免物理内存使用完后，造成卡顿的情况。并且线上要避免大量key同时失效的场景，因为redis删除失效的key是循环删除的，并且频繁的删除会促使内存管理器回收内存页，这样也会导致卡顿的现象。<br>
6、connection客户端连接数量，Redis作为一个事件驱动模型，因为base epoll能够实现O(1)时间复杂度的响应操作，因此能够提供很好性能。Redis已经以超过60000个连接为基准，并且在这些条件下仍能够维持50000 q / s的吞吐量，而且具有30000个连接的实例只能处理100个连接可达到的吞吐量的一半，可参考下图（来源官网）：<br>
<img src="https://zhangyaoo.github.io/post-images/1592991673063.png" alt="" loading="lazy"></p>
<h5 id="elasticsearch">ElasticSearch</h5>
<p>ElasticSearch可以解决大数据量下的搜索慢问题，这里笔者就拿 死磕ElasticSearch社区作者的优化建议，给出几点在日常开发ElasticSearch的优化方案：<br>
1、尽量将所有数据的一半都缓存在内存当中file cache system 当中<br>
2、将少量（查询字段比较频繁）字段放入ES，其他全量字段放入Hbase中，采用ES + Hbase方式提升查询效率，节省ES存储空间，file cache system的数据就会存的更多<br>
3、缓存预热，可以做一个缓存预热系统，定时查询热点数据将其缓存在filesystem cache 中<br>
4、冷热分离，将访问量高的和冷数据分别放置索引<br>
5、ES ducoment设计，尽量避免连接、父子文档等连接操作，将数据准备好后再存入ES<br>
6、不允许深度分页，页数越大，深度越深，从每一个shard返回的数据就越多，耗时越久。可以通过scroll api游标进行查询。<br>
7、必须限制模糊搜索的长度，不然CPU会飙高，可参考 https://elasticsearch.cn/article/171</p>
<h3 id="3-业务层方面">3、业务层方面</h3>
<p>每个公司业务层面优化不相同，要根据具体业务场景去优化，别人的方案只能作为参考。<br>
这里笔者就拿金融行业背景下，列举三个优化例子。<br>
1、背景：企业借贷，会从用户的投的银行某个产品的资金池中匹配查找合适的资金，然后进行资金占有，银行真实转账后，生成终态的债权关系。<br>
优化之前：使用同步锁，同一时间只能又一个线程去资金池中匹配资金，这种方案有严重的性能问题，其他线程没有拿到锁之前只能自旋尝试获取锁，损耗CPU资源。<br>
优化之后：去掉同步锁，改成乐观锁，放到数据库做，资金表的字段增加一个标识表示是否占用，线程进来尝试匹配资金，乐观锁去预占资金表，成功表示匹配成功。这里要注意的是，尽量一个借贷匹配一比资金，这样资金池里面的资金锁行范围会减少（因为有多线程抢占资金资源），资金匹配的速度会加快，并且这样优化这样银行转账的次数会减少（如果匹配一批资金就要进行相同数量转账次数），防止多个投资人账户进行银行转账，减少整个借款业务线的耗时，避免其中一个转账出错导致全部要回滚这样的情况。<br>
这里其实还可以进行优化，比如一笔一笔的占有资金，占有失败的continue继续下一笔资金含有，不用一次在一个事务里面占有大量资金，防止大事务执行失败以及出现其他会有死锁的可能性。这里优化的思想就是大事务拆成小事务，防止事务执行失败的概率。</p>
<p>2、背景：企业借贷，同步请求转异步<br>
优化之前，三方企业借贷请求，是同步调用，因为一条完整的借贷业务线非常长，中间会RPC调用非常多的底层服务以及其他远程接口，这样的话请求到响应时间会拉的非常长，影响C端的用户体验。<br>
优化之后，同步改异步，具体做法是，借贷请求进入系统后，会先生成一个进行中的状态借贷数据插入数据库，然后将唯一标识丢入MQ中，然后就返回成功。这样吞吐量会增加，用户端体验会非常好。<br>
当然如果说要保证高可用，可以利用MQ的事务消息做，利用二阶段提交方式保证MQ能够收到消息。具体方案可以看笔者的这篇文章 <a href="https://zhangyaoo.github.io/post/jie-jue-bing-fa-xia-ben-di-shi-wu-he-xiao-xi-tou-di-yi-zhi-xing/">本地事务消息投递一致性</a></p>
<p>3、背景：C端用户在APP上，某一个标开启募集资金后进行投标，这里的标类似支付宝的理财产品，开始募集的时候会有高并发流量涌入，当APP端用户同时投标，会有大量请求，这就形成了抢购的动作，因为一个产品标的可投金额是有限的，只有少数人能投标成功<br>
优化之前：笔者在上文中提到的，尽管说频繁将更新行锁的数据放到事务的最后， 会有性能提升，但是随着并发数增长，MySQL也会成为性能瓶颈。<br>
优化之后：利用redis的纯内存操作高性能的优点，将产品的可投金额放入缓存redis（这里redis里面的金额比数据库中少保证不超卖），利用redis的decrby命令或者lua脚本，保证产品标剩余可投金额能够进行原子减少，每一次减少成功后，将用户这一次投标的数据丢入MQ中异步处理，消费端做的就是将插入预状态的借款数据、冻结用户金额和减少产品的可投金额放入同一个事务中处理。<br>
上面做的优化能够保证C端用户的良好体验，但是引入各种中间件的话会出现各种问题需要去解决，比如重复投标怎么办，Redis挂了怎么？MQ挂了怎么办？消息丢失怎么办？等一系列问题。<br>
重复投标，可以利用用户标识的唯一token做，短信生成token，设置token失效时间（短信失效时间60s），投标时候校验token是否过期和使用过。<br>
Redis挂，这时候就要考虑持久化和集群哨兵保持redis高可用。<br>
MQ挂了，消息丢失，重复消费等，这时候就要考虑broker的持久化，生产端和消费端的重试机制和ack机制。<br>
以上需要开发人员去应对每一个可能出现问题的场景。</p>
<h2 id="三-后续流量增长系统性能优化思路">三、后续流量增长系统性能优化思路</h2>
<p>当流量激增的时候，首先要考虑到系统的稳定性和高可用，后续针对特定的场景，分析性能瓶颈，然后再去做并发的优化。这里笔者就<strong>简单的</strong>列举一下业界的做法，下面每一条读者都可以自行扩展大篇幅深入去了解。</p>
<h4 id="高可用">高可用</h4>
<ol>
<li>使用反向代理和<strong>负载均衡</strong>实现分流，并且实现动态切换主备机器， 网关负载均衡，DNS多机房负载均衡</li>
<li>通过<strong>限流</strong>保护应用免受雪崩之灾</li>
<li>通过<strong>降级</strong>实现核心服务服务可用，牺牲非核心服务</li>
<li>通过<strong>隔离</strong>实现故障隔离和资源隔离，比如线程隔离，对方法或者类具体分配线程数量，防止相互影响</li>
<li>通过设置合理的<strong>超时</strong>调用与重试机制避免请求堆积造成雪崩</li>
<li>通过<strong>回滚</strong>机制快速修复错误版本</li>
<li>Redis<strong>集群</strong>保证高可用，<strong>哨兵</strong>模式保证故障转移</li>
<li>Redis MQ消息中间件开启<strong>持久化</strong>，保证数据不丢；<strong>ack机制</strong>和<strong>重试机制</strong>保证数据的可靠性</li>
<li>分布式服务环境<strong>链路跟踪</strong>，监控整个服务的服务质量</li>
<li>硬件资源<strong>监控</strong>、CPU、内存、负载、IO、堆内存、JVM GC、线程池以及各种中间件监控等等</li>
</ol>
<h4 id="高并发">高并发</h4>
<ol>
<li>利用MQ同步转异步，流量削峰，多线程异步消费</li>
<li>读场景比较多的接口可以利用缓存Redis，包括热点key缓存预热，多级缓存，比如分布式缓存，本地缓存和CDN缓存。还可以做集群、哨兵、高可用保证Redis性能</li>
<li>对于强一致的同步下单场景，可以将对MySQL的操作改为Redis，然后利用MQ做异步</li>
<li>优化JVM，包括新生代和老年代的大小、GC算法的选择等，尽可能减少GC频率和耗时</li>
<li>非核心业务逻辑、延迟任务逻辑、三方调用逻辑，可以做异步</li>
<li>对于架构方面，可以做负载均衡集群部署，MySQL主从、分库分表或者归档、Redis集群、多级缓存、分布式垂直拆分部署、搜索场景引入ES等多个方面考虑</li>
<li>对于程序方面，可以从For循环的计算逻辑优化、批处理机制减少IO、采用时间复杂度更小的数据结构和算法、乐观锁和分段锁和无锁编程减少锁冲突等方面考虑</li>
</ol>
<h2 id="四-总结">四、总结</h2>
<p>以上，笔者从硬件层、代码层、中间件层、业务层等不同方向，简单的分析了影响系统性能各个因素，以及提供了简单的优化的思路和例子。因笔者工作经验能力有限，无法做到全面的分析，还望读者能够指正错误以及提供建议。</p>
<h2 id="参考">参考</h2>
<p>1、秒杀场景下MySQL的低效——丁奇<br>
2、死磕ElasticSearch社区——ElasticSearch优化<br>
3、MySQL性能压测——https://my.oschina.net/u/867417/blog/758690<br>
4、高可用系统方案——https://blog.csdn.net/hustspy1990/article/details/78008324<br>
5、Redis性能测试——https://redis.io/topics/benchmarks</p>
<!--参考 -->
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[O(1)时间复杂度下，双向链表实现LRU]]></title>
        <id>https://zhangyaoo.github.io/post/o1shi-jian-fu-za-du-xia-shuang-xiang-lian-biao-shi-xian-lru/</id>
        <link href="https://zhangyaoo.github.io/post/o1shi-jian-fu-za-du-xia-shuang-xiang-lian-biao-shi-xian-lru/">
        </link>
        <updated>2020-06-15T11:17:45.000Z</updated>
        <content type="html"><![CDATA[<h3 id="前言">前言</h3>
<blockquote>
<p>笔者先从linkedHashMap源码中借鉴插入顺序访问的代码，然后然后自己实现了一个LRU</p>
</blockquote>
<h3 id="linkedhashmap底层的数据结构">linkedHashMap底层的数据结构</h3>
<p>linkedHashMap底层结构（顺序访问）：</p>
<ul>
<li>1、linkedHashMap维护了每个node的双向链表，初始化时候维护了空的entry header头，新加入的节点放到entry的头部header的next</li>
<li>2、put还是get都会进行重排序，get entry1 还是put entry1都会先把Entry1从双向链表中删除，然后再把Entry1加入到双向链表的表尾。</li>
<li>3、遍历访问的时候，会访问header的下一个next节点，这就形成了顺序访问</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://zhangyaoo.github.io/post-images/1592968145044.png" alt="" loading="lazy"></figure>
<h3 id="链表实现">链表实现</h3>
<p>实现思路：</p>
<ul>
<li>1、数据是直接利用 HashMap 来存放的。</li>
<li>2、内部使用了一个双向链表来存放数据，所以有一个头结点 header，以及尾结点 tailer。</li>
<li>3、每次写入头结点，删除尾结点时都是依赖于 header tailer<br>
<img src="https://zhangyaoo.github.io/post-images/1592220507945.png" alt="" loading="lazy"></li>
</ul>
<pre><code>import com.google.common.collect.Maps;
import java.util.Map;

/**
 * 线程不安全，同步机制自行控制。
 */
public class LRUCacheV2 {
    /**
     * 缓存map
     */
    private final Map&lt;String, Node&gt; cacheMap;

    /**
     * 头指针
     */
    private Node head;

    /**
     * 尾指针
     */
    private Node tail;

    /**
     * 容量
     */
    private final int cacheSize;

    /**
     * 当前容量
     */
    private int currentCacheSize;

    LRUCacheV2(int capacity){
        cacheMap = Maps.newHashMapWithExpectedSize(capacity);
        cacheSize = capacity;
        currentCacheSize = 0;
    }

    public Object get(String key){
        Node node = cacheMap.get(key);
        if(node != null){
            // 移动到头指针
            move2head(node);
            return node.getData();
        }
        return null;
    }

    public void remove(String key){
        Node node = cacheMap.get(key);
        if(node != null){
            Node pre = node.getPre();
            Node next = node.getNext();
            if(pre != null){
                pre.setNext(next);
            }
            if(next != null){
                next.setPre(pre);
            }

            // 如果删除刚好是头节点或者尾节点，也要移动指针
            if(node.getKey().equals(head.getKey())){
                head = pre;
            }
            if(node.getKey().equals(tail.getKey())){
                tail = next;
            }

            cacheMap.remove(key);
        }
    }

    public void put(String key, Object value){
        Node node = cacheMap.get(key);
        if(node != null){
            // 存在节点的话，就覆盖，并且放到头
            node.setData(value);
            move2head(node);
            cacheMap.put(key, node);
        }else {
            // 不存在节点，构造并且放到头
            if(currentCacheSize == cacheSize){
                // 删除尾node
                String delKey = tail.getKey();
                cacheMap.remove(delKey);

                // 尾指针移动
                Node next = tail.getNext();
                if(next != null){
                    next.setPre(null);
                }
                tail.setNext(null);
                tail = next;

            }else{
                currentCacheSize++;
            }
            node = new Node();
            node.setData(value);
            node.setKey(key);
            // 头指针移动
            move2head(node);
        }
        cacheMap.put(key, node);
    }

    /**
     * 节点移到头
     */
    private void move2head(Node node){
        if(head == null){
            // 初始化head 和 tail
            head = node;
            head.setNext(null);
            head.setPre(null);
            tail = node;
        }else {
            // 如果是相同的Key，啥都不用动，node就是最新的头
            if(node.getKey().equals(head.getKey())){
                return;
            }

            // 截取node
            Node pre = node.getPre();
            Node next = node.getNext();
            if(pre != null){
                pre.setNext(next);
            }
            if(next != null){
                next.setPre(pre);
            }

            // 如果要截取的节点是尾节点，那么尾节点指针也要向前移动
            if(node.getKey().equals(tail.getKey())){
                tail = next;
            }

            // 放在头前面
            head.setNext(node);
            node.setPre(head);
            // node下个指针指向null
            node.setNext(null);
            head = node;
        }
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder() ;
        Node node = head;
        while (node != null){
            sb.append(node.getKey()).append(&quot;:&quot;)
                    .append(node.getData())
                    .append(&quot;--&gt;&quot;) ;
            node = node.getPre();
        }
        return sb.toString();
    }

    public static void main(String[] args) {
        LRUCacheV2 lruCacheV2 = new LRUCacheV2(4);
        lruCacheV2.put(&quot;1&quot;,&quot;1&quot;);
        lruCacheV2.put(&quot;2&quot;,&quot;2&quot;);
        lruCacheV2.put(&quot;3&quot;,&quot;3&quot;);
        lruCacheV2.put(&quot;4&quot;,&quot;4&quot;);
        lruCacheV2.put(&quot;5&quot;,&quot;5&quot;);
        //lruCacheV2.get(&quot;2&quot;);
        //lruCacheV2.put(&quot;2&quot;,&quot;22&quot;);
        lruCacheV2.remove(&quot;5&quot;);
        System.out.println(lruCacheV2.toString());
    }
}
</code></pre>
<p>参考：<br>
1、https://www.iteye.com/blog/gogole-692103<br>
2、https://crossoverjie.top/2018/04/07/algorithm/LRU-cache/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[关于]]></title>
        <id>https://zhangyaoo.github.io/post/about/</id>
        <link href="https://zhangyaoo.github.io/post/about/">
        </link>
        <updated>2019-01-25T11:09:48.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>欢迎来到我的小站呀，很高兴遇见你！🤝</p>
</blockquote>
<h2 id="关于本站">🏠 关于本站</h2>
<p>一些技术博客和一些闲言碎语</p>
<h2 id="博主是谁">👨‍💻 博主是谁</h2>
<p>还在打怪升级的菜鸟</p>
<h2 id="兴趣爱好">⛹ 兴趣爱好</h2>
<p>二次元、骑行、篮球、LOL、编码</p>
<h2 id="联系我呀">📬 联系我呀</h2>
<p>QQ：1124826889</p>
]]></content>
    </entry>
</feed>