<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://zhangyaoo.github.io</id>
    <title>will</title>
    <updated>2021-02-04T08:31:21.093Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://zhangyaoo.github.io"/>
    <link rel="self" href="https://zhangyaoo.github.io/atom.xml"/>
    <subtitle>生死看淡，不服就干</subtitle>
    <logo>https://zhangyaoo.github.io/images/avatar.png</logo>
    <icon>https://zhangyaoo.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, will</rights>
    <entry>
        <title type="html"><![CDATA[记一次多数据源路由造成的数据库连接泄露排查过程]]></title>
        <id>https://zhangyaoo.github.io/post/ji-yi-ci-duo-shu-ju-yuan-lu-you-zao-cheng-de-shu-ju-ku-lian-jie-nei-cun-xie-lu/</id>
        <link href="https://zhangyaoo.github.io/post/ji-yi-ci-duo-shu-ju-yuan-lu-you-zao-cheng-de-shu-ju-ku-lian-jie-nei-cun-xie-lu/">
        </link>
        <updated>2021-02-02T03:32:16.000Z</updated>
        <content type="html"><![CDATA[<h3 id="前言">前言</h3>
<p>之前一篇文章讲了自己写了一个多数据源路由组件给公司内部使用，进行快速迭代。文章URL是 <a href="https://zhangyaoo.github.io/post/saas-xi-tong-duo-shu-ju-yuan-lu-you-you-ya-jie-jue-fang-an">SaaS系统多数据源路由优雅解决方案</a><br>
随着时间推移，某一天运维找上门说数据库连接打满，就刚好这一台机器上装了Java服务导致的。<br>
下面文章就是讲的这次连接泄露导致的数据库hang住问题以及后面的解决过程。</p>
<h3 id="一-案发当时的情况">一、案发当时的情况：</h3>
<p>线上MySQL session 逐渐增加，不活跃的的数量逐渐增加，导致的后果：占用链接，导致链接满了无法分配新的连接<br>
<img src="https://zhangyaoo.github.io/post-images/1612261925869.jpg" alt="" loading="lazy"></p>
<h3 id="二-紧急处理方式">二、紧急处理方式：</h3>
<p>用自研的应用层网关将流量切换为旧的版本中，并且不停机切换，然后将当前的版本的服务停调，然后观察阿里云的数据库连接数量变回正常。</p>
<p>目前线上存在多个版本存在docker，然后用K8s部署，实现快速不停机切换上线。<br>
自研网关这篇文章有介绍： <a href="ying-yong-ceng-wang-guan-she-ji-yu-shi-xian">应用层网关设计和实现</a></p>
<h3 id="三-第一次查找问题并且解决的过程">三、第一次查找问题并且解决的过程：</h3>
<p>1、开启本地服务，让它飞一会，或者多线程并且切租户查询数据库，dump内存快照进行分析<br>
<img src="https://zhangyaoo.github.io/post-images/1612253583270.png" alt="" loading="lazy"></p>
<p>2、然后查看本地服务相关数据库连接的对象<br>
1）分析sqlsession 对象，发现sqlsession对象为0，而且日志中有打印及时关闭sqlsession。结论：sqlsession正常，说明正常关闭了sqlsession<br>
<img src="https://zhangyaoo.github.io/post-images/1612237357285.png" alt="" loading="lazy"><br>
2）分析datasource 对象，发现datasource对象和数据库连接配置中的max-pool-size一致。结论：datasource正常，说明正常关闭了datasource<br>
<img src="https://zhangyaoo.github.io/post-images/1612237380060.png" alt="" loading="lazy"><br>
3）分析connection对象，发现HikariProxyConnection对象和数据库配置一致。结论：HikariProxyConnection正常<br>
<img src="https://zhangyaoo.github.io/post-images/1612237411657.png" alt="" loading="lazy"></p>
<p>3、在分析本地内存快照，发现一个类随着时间的推移，逐渐增多，并且没有被回收，正是connectionImpl对象，这个对象是MySQL底层连接的实现，来自com.mysql.cj.jdbc<br>
<img src="https://zhangyaoo.github.io/post-images/1612237433473.png" alt="" loading="lazy"><br>
（图片中指的类有100多个对象）</p>
<p>4、跟踪了一波源码，发现HikariProxyConnection对象，实质上底层就是new了一个并管理connectionImpl对象，猜测某个因为参数原因导致HikariProxyConnection及时释放，而connectionImpl没有释放，积累没有及时清除导致的。</p>
<p>5、经过Google查找问题，怀疑是max_lifetime导致的问题。max_lifetime官网解释：一个连接的生命时长（毫秒），超时而且没被使用则被释放（retired）。建议比数据库的wait_timeout小几分钟。默认30分钟<br>
<img src="https://zhangyaoo.github.io/post-images/1612237640791.png" alt="" loading="lazy"></p>
<p>6、查看线上配置和测试环境配置，果然是只配置了1分钟。<img src="https://zhangyaoo.github.io/post-images/1612262047685.jpg" alt="" loading="lazy"><br>
随后将其改成30分钟，然后继续多线程并发跑测试用例。测试环境验证，dump内存，观察connectionImpl对象并没有随着时间的推移增加。发现connectionImpl对象并没有随着时间的推移增加。验证了个人的猜想。<br>
<img src="https://zhangyaoo.github.io/post-images/1612237699007.png" alt="" loading="lazy"></p>
<p>7、最终将配置更新后上线，过了没2个小时，连接数又飙升。初步观察还是connectionImpl对象增多，连接没有释放。所以认为，不是max_lifetime配置的问题。还是得从源码中入手和线上的dump入手看。</p>
<p>第一次查找问题并<strong>没有解决根本问题</strong>，对此总结了一下：</p>
<ol>
<li>自己的猜想缺少实际的数据支持和多方位的有力证明</li>
<li>对源码研究不够深入，只是停留在表明</li>
<li>对官网的配置参数理解不够透彻</li>
</ol>
<h3 id="四-第二次查找问题并且解决的过程">四、第二次查找问题并且解决的过程：</h3>
<p>本地环境观察没有问题，正式环境观察就有问题。像这种问题算是比较难解决的，为了快速解决问题，避免把线上数据copy到本地进行测试，直接down线上的dump文件下来进行仔细分析。</p>
<p>1、查找线上的内存，dump数量，刚好100个，并且随着时间偏移，这个类对象越来越多，个人猜想就是这个对象没有被回收导致连接数未释放。有强引用引用这个对象。<br>
<img src="https://zhangyaoo.github.io/post-images/1612420897419.png" alt="" loading="lazy"></p>
<p>2、查找这个对象GC root对象，右键选择Merge Shortest Paths to GC roots -&gt; exclude all phantom/weak/soft etc.reference(排除所有虚弱软引用)，发现这100个对象大部分被一个名字叫做housekeeper线程所强引用，如下图。个人验证猜想是线程没有及时回收关闭或者是没有关闭线程池。<br>
<img src="https://zhangyaoo.github.io/post-images/1612423498302.png" alt="" loading="lazy"></p>
<p>3、这个时候dump文件里面就没法发现更多有用的信息了，然后就去看源码看这个housekeeper线程为什么一直强引用。查找源码发现在获取datasource 的 getConnetcion方法，会初始化HikariPool连接池。<br>
<img src="https://zhangyaoo.github.io/post-images/1612423896839.png" alt="" loading="lazy"></p>
<p>初始化连接池里面会初始化housekeeper连接池，刚好对应了上面在根引用的对象。<br>
<img src="https://zhangyaoo.github.io/post-images/1612423966034.png" alt="" loading="lazy"><br>
<img src="https://zhangyaoo.github.io/post-images/1612425624943.png" alt="" loading="lazy"></p>
<p>看了下这个housekeeper对象的作用，目的是为了维持最少的空闲的连接，说白了就是根据配置参数idleTimeout和minIdle来维持最小的空闲连接数。<br>
<img src="https://zhangyaoo.github.io/post-images/1612425545450.png" alt="" loading="lazy"></p>
<p>4、至此，可以初步得出结论，是由于应用程序不停的new HikariPool，然后没有及时close导致的问题。然后顺着这个结论去查看应用程序的BUG</p>
<p>5、在程序发现下面逻辑，下面这一段逻辑就是根据租户来获取缓存中租户对应的datasource对象</p>
<pre><code class="language-java">   /**
     * 获取已缓存的数据源
     */
    private Optional&lt;DataSource&gt; getDataSourceIfCache(SelectRdsConfigDto rdsConfigDto) {
        String key = getUniqueMysqlKey(rdsConfigDto.getHost(), rdsConfigDto.getPort());
        if (Objects.nonNull(dataSourceCachePool.get(key))) {
            DataSourceCache dataSourceCache = dataSourceCachePool.get(key);
            //cache中已经缓存了租户的连接并且没有修改rds config信息
            if (dataSourceCache.verifyRdsConfig(rdsConfigDto)) {
                return Optional.ofNullable(dataSourceCachePool.get(key).getDataSource());
            }
            //cache中已经缓存了租户的连接，但是校验不通过
            else {
                dataSourceCachePool.remove(key);
                return Optional.empty();
            }
        }
        return Optional.empty();
    }

   // 校验
   private boolean verifyRdsConfig(SelectRdsConfigDto rdsConfigDto) {
        return rdsConfigDto.getAccount().equals(this.account) &amp;&amp;
                rdsConfigDto.getHost().equals(this.host) &amp;&amp;
                rdsConfigDto.getPort().equals(this.port) &amp;&amp;    rdsConfigDto.getPwd().equals(this.pwd) &amp;&amp;；

    //  dataSourceCachePool的key组成，一个MySQL连接对应的key
    private String getUniqueMysqlKey(String host, Integer port){
        return host + &quot;:&quot; + port;
    }

 }
</code></pre>
<p>这里逻辑有一段是校验不通过，会remove对应的datasource对象，问题就出现在这里，这里没有及时close。校验不通过的原因就是，一个host port作为一个dataSourceCachePool的key，因为线上有一个MySQL实例多个账号，导致校验总是不通过误以为成是修改了用户名或者密码，随后就是一直new datasource对象。</p>
<h3 id="五-根本原因">五、根本原因</h3>
<p>所以综上，最终得出结论是，因为程序问题导致HikariDataSource对象增多，而且因为HikariDataSource对象内部有一个线程池，如果外部丢失了对这个HikariDataSource对象的引用，也不会被垃圾回收，导致HikariDataSource对象不释放，然后结果就是数据库连接未释放。</p>
<h3 id="六-最终处理方式">六、最终处理方式：</h3>
<p>1、修改代码：<br>
1） dataSourceCachePool的key组成由host port 改成 host port account pwd 四个维度作为一个key。</p>
<p>2）关闭datasource对象，remove那段代码的逻辑修改成下面这个样子</p>
<pre><code class="language-java">//cache中已经缓存了租户的连接,但是修改了rds config信息
        DataSource dataSource = dataSourceCachePool.remove(key).getDataSource();
        log.info(&quot;remove datasource:{}, {}&quot;, key, rdsConfigDto.toString());
        new Thread(() -&gt; {
            while (true) {
                try {
                    TimeUnit.SECONDS.sleep(10);
                } catch (InterruptedException e) {
                    log.error(&quot;e:&quot;, e);
                }
                if (dataSource instanceof HikariDataSource) {
                    if (((HikariDataSource) dataSource).getHikariPoolMXBean().getActiveConnections() &gt; 0){
                        log.info(&quot;ActiveConnections &gt; 0, continue&quot;);
                        continue;
                    }
                    log.info(&quot;HikariDataSource close...&quot;);
                    ((HikariDataSource) dataSource).close();
                } else if (dataSource instanceof DruidDataSource) {
                    if (((DruidDataSource) dataSource).getActiveCount() &gt; 0) {
                        log.info(&quot;ActiveConnections &gt; 0, continue&quot;);
                        continue;
                    }
                    log.info(&quot;DruidDataSource close...&quot;);
                    ((DruidDataSource) dataSource).close();
                } else {
                    log.error(&quot;close datasource|datasource is wrong&quot;);
                    throw new RuntimeException(&quot;close datasource|datasource is wrong&quot;);
                }
                break;
            }
        }).start();
</code></pre>
<p>Q：为什么要判断活跃的连接 ActiveCount &gt; 0 ？<br>
A：因为close的时候要判断是否有正在使用的connection对象，如果强制关闭，那么会出现一个线程查询的时候，connetion突然不可用，导致错误。</p>
<p>2、改配置，改成和官方默认的配置<br>
<img src="https://zhangyaoo.github.io/post-images/1612426469625.png" alt="" loading="lazy"></p>
<p>最后，重新上线后，观测对象内存数据，正常。</p>
<h3 id="七-总结">七、总结</h3>
<p>底层的数据库组件的代码编写，要注意使用数据库连接资源的时候，一定要检查代码中是否释放，不然会造成严重的事故。而且平常还要熟悉官方配置，并且多研究源码。以免出现这种情况时束手无策。<br>
可以的话还可以叫身边的资深大牛给review代码，做到双重保障。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[2020年总结]]></title>
        <id>https://zhangyaoo.github.io/post/2020-nian-zong-jie/</id>
        <link href="https://zhangyaoo.github.io/post/2020-nian-zong-jie/">
        </link>
        <updated>2020-12-31T03:59:36.000Z</updated>
        <content type="html"><![CDATA[<figure data-type="image" tabindex="1"><img src="https://zhangyaoo.github.io/post-images/1609517863900.jpg" alt="" loading="lazy"></figure>
<h3 id="开头">开头</h3>
<p>今天是1月1号，新年第一天，抓住2020年的小尾巴，随便絮絮叨叨一下，做个小总结。回想一下今年一整年，开心，困难，心酸，大部分都经历过，没有什么值得说的经历，倒是小事挺多。</p>
<p>值得提的事情就是疫情了，今年疫情在家呆了2个月，那时候因为年前被裁，已经是失业的状态，没有收入来源，开始坐立不安，烦躁 。后面意识到了在家也有在家值得做的事情，不必因为疫情打乱自己的内心，充实陪家人过每一天。</p>
<p>作为成年人，现在认为时常开心快乐倒是一件很难的事情，想起一句话，小时候快乐是一件简单的事情，长大后，简单是一件快乐的事情。不过后面还是要时常开心为主，然后就是今年以后要沉静下来，理性思考，不管对人还是对事。还要多看些关于生活上的书，不只是盯着技术书死啃，提醒一下自己要停下来多思考，低头做事，抬头看路。</p>
<p>今年总结方面有些少，因为我自己认为这些方面值得去总结，至于其他的，可能我没有意识到或者是今年经历事情的太少。</p>
<h3 id="学习">学习</h3>
<p><img src="https://zhangyaoo.github.io/post-images/1609517630829.jpg" alt="" loading="lazy"><br>
关于学习这件事，还是要放在第一来说。学无止境，活到老学到老。人不能总是停留在原地，还是要多看书多学习，增长知识。中国人大部分劳动人民以前（80年代）是基本靠努力，靠勤奋，获得属于自己的劳动果实，现在信息时代，除了靠努力勤奋以外，还要有效利用专业知识去解决生活工作中的问题，享受现在信息时代给与的成果。<br>
就像文臣武将，谋士当出谋划策，武士当战场厮杀。每个人当去发挥自己的专业本领。不管最终目的是啥，精忠报国还是养家糊口，小有小义，大有大义。</p>
<p>扯远了，今年准备还是以技术学习为主，然后以公众号和GitHub为辅助进行学习。<br>
今年准备看完这些学习资料，其中有书籍以及网络课程<br>
1、MySQL是如何运行的<br>
2、redis设计和实现<br>
3、架构师视频课<br>
4、极客时间MySQL<br>
5、Netty网络编程学习+技术书籍+Netty实战</p>
<p>不多，列多了就看不完了[dog]。</p>
<p>其他则按时及时总结工作上遇到的问题，提升自己的技术深度。毕竟工作中遇到的最容易加深理解的和提升能力的。<br>
顺便看一些优秀的博客及时总结，输出自己的理解。然后维护到自己的个人网站上面去。</p>
<p>然后其他时间，准备看一些其他类型的书籍，比如人文历史，以及其他优秀的国内外著作，现在已经想好看些什么，就不一一列举了，今年今后会抽空余时间去看这些，提升自己对人和生活的一些思考。</p>
<p>最后的话要提醒自己少看动漫二次元多学习，早睡早起~~</p>
<h3 id="运动">运动</h3>
<p><img src="https://zhangyaoo.github.io/post-images/1609517659211.jpg" alt="" loading="lazy"><br>
运动这件事情，算是我个人每年立的flag中，自己最满意的了。一周最少一次跑步+篮球。基本上也不算全部，每周都做了。<br>
我身边的朋友或者亲人，我会经常提醒他们，要适当的锻炼。虽然总是被当作耳边风，但是每次过年祝别人身体健康只是噱头，只有真正关心你的人会在乎你的健康。我个人不管是生日还是新年愿望，大部分都是希望身边的人过的平平安安开开心心就行了，其他都是扯淡。<br>
而我自己运动的动力来源是，如果我自己身体不行，那就没有能力照护身边人。我自己能够成长为大树，让别人安心倚靠。</p>
<p>现在虽然毕业了3年，6块腹肌已经变成了4快，身体没有毕业那会好，但是我自己还是会坚持的，朝着更man的目标奋斗。想当年我可是拥有8块腹肌的男人。。。。</p>
<h3 id="爱情">爱情</h3>
<p><img src="https://zhangyaoo.github.io/post-images/1609517842818.jpg" alt="" loading="lazy"><br>
今年最开心的就是和女朋友度过第一年的恋爱期（不要脸的我，自认为网恋了一年的热恋期），这一年有开心和难过，不过难过时刻很少，大部分都是开心时刻，我觉得已经可以了，能够和对方每天都很开心，已经很不错了。毕竟和喜欢的人在一起不就是开心撒，这个是最难能可贵的。。</p>
<p>我自认为我是个不好i相处的人，个人的小脾气很多，而且有时候会常有不自信。谢谢我女朋友的理解和鼓励。今年往后，还希望一起开心度过每一天，哈哈哈哈。</p>
<p>新的一年，希望咱俩都身体健康，开开心心~</p>
<h3 id="2021flag">2021flag</h3>
<p>国际惯例，最后说一些打脸的flag<br>
1、完成上面的资料的学习<br>
2、每个月一次文章总结，工作总结，4次个人网站文章的发表和维护<br>
3、一周一次运动+篮球<br>
4、身体体重不超130<br>
5、经营好个人博客<br>
6、 早睡早起，11点半之前睡</p>
<p><strong>2021年，干就完事</strong>。<br>
<img src="https://zhangyaoo.github.io/post-images/1609517948326.jpg" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[应用层网关设计与实现]]></title>
        <id>https://zhangyaoo.github.io/post/ying-yong-ceng-wang-guan-she-ji-yu-shi-xian/</id>
        <link href="https://zhangyaoo.github.io/post/ying-yong-ceng-wang-guan-she-ji-yu-shi-xian/">
        </link>
        <updated>2020-12-22T03:12:25.000Z</updated>
        <content type="html"><![CDATA[<p>##还没完成，敬请期待</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[概率相等的拼手气红包算法实现]]></title>
        <id>https://zhangyaoo.github.io/post/shou-lu-yi-ge-hong-bao-suan-fa/</id>
        <link href="https://zhangyaoo.github.io/post/shou-lu-yi-ge-hong-bao-suan-fa/">
        </link>
        <updated>2020-12-17T14:17:09.000Z</updated>
        <content type="html"><![CDATA[<h3 id="1-要求">1、要求</h3>
<p>红包算法 function：拼手气红包<br>
1、每个红包获得的数学概率要一样<br>
2、红包最小值：1分钱</p>
<h3 id="2-思路">2、思路：</h3>
<p>利用切片的思想，比如有5个红包，20分，就从0-20的范围内获取四个随机数，就被分成5份，然后顺序抽取。这样保证每个红包的的概率一样<br>
这种方案需要考虑到几个注意点：<br>
1）重复的随机切片如何处理；<br>
2）需要考虑1分钱如何处理；</p>
<h3 id="3-实现">3、实现：</h3>
<p>遇到随机切片重复:重新生成切片直至不重复<br>
1分钱处理：判断相邻间隔的大小</p>
<p>###4、 代码实现：<br>
利用treeMap的顺序有序性</p>
<p>直接上代码（笔者已经调试过没有问题，可以运行）</p>
<pre><code class="language-java">public class RedPackage {
   /**
    * 红包最小金额
    */
   private long minAmount = 1L;

   /**
    * 最大的红包是平均值的N倍
    */
   private static final long N = 2;

   /**
    * 红包最大金额
    */
   private long maxAmount;

   /**
    * 红包金额 分
    */
   private long packageAmount;

   /**
    * 红包个数
    */
   private long packageSize;

   /**
    * 是否抢完
    */
   private boolean finish;

   /**
    * 存储红包的金额顺序
    */
   private final TreeMap&lt;Long, Long&gt; treeMap = Maps.newTreeMap((o1, o2) -&gt; o1 &gt; o2 ? 1 : o1.equals(o2) ? 0 : -1);

   /**
    * 构造函数不写业务逻辑
    */
   public RedPackage(long packageAmount, int packageSize){
       this.packageAmount = packageAmount;
       this.packageSize = packageSize;
       maxAmount = (packageAmount * N)/ packageSize;
   }

   public RedPackage(long packageAmount, int packageSize, long minAmount){
       this.packageAmount = packageAmount;
       this.packageSize = packageSize;
       this.minAmount = minAmount;
   }

   /**
    * 获取金额
    */
   public synchronized long nextAmount(){
       // 前置校验，初始化
       if(!finish &amp;&amp; treeMap.size() == 0){
           treeMap.put(packageAmount, 0L);
           for (int i = 0; i &lt; packageSize - 1; i++) {
               // 随机抽取切片
               long splitNum = RandomUtils.nextLong(minAmount, packageAmount);
               Long higher = treeMap.higherKey(splitNum);
               higher = higher == null ? packageAmount : higher;
               Long lower = treeMap.lowerKey(splitNum);
               lower = lower == null ? 0 : lower;
               // 相同切片重新生成,和上一个或者下一个切片间隔小于minAmount的重新生成
               while (higher - splitNum &lt;= minAmount
                       || splitNum - lower &lt;= minAmount
                       || treeMap.containsKey(splitNum)){
                   splitNum = RandomUtils.nextLong(minAmount, packageAmount);
               }
               // value放入上一个entry的key,组成链条，防止再次循环
               treeMap.put(splitNum, lower);
               treeMap.put(higher, splitNum);
           }
           System.out.println(&quot;init finish&quot;);
       }
       Map.Entry&lt;Long, Long&gt; entry = treeMap.pollFirstEntry();
       if(treeMap.size() == 0){
           // 用完红包
           this.finish = true;
           if(entry == null){
               return 0L;
           }
       }
       return entry.getKey() - entry.getValue();
   }

   public static void main(String[] args) {
       RedPackage redPackage = new RedPackage(1500L, 10, 10L);
       long result = 0;
       for (int i = 0; i &lt; 15; i++) {
           long amount = redPackage.nextAmount()；
           System.out.println(amount);
           result = result + amount;
       }
       System.out.println(result);
   }
}
</code></pre>
<h3 id="5-todo-设置单个红包最大限额值">5、TODO 设置单个红包最大限额值</h3>
<p>目前还没有实现思路</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[记一次用户中心采用分布式ID踩的坑]]></title>
        <id>https://zhangyaoo.github.io/post/ji-yi-ci-cai-yong-fen-bu-shi-id-jie-jue-fen-ku-fen-biao-cai-de-keng/</id>
        <link href="https://zhangyaoo.github.io/post/ji-yi-ci-cai-yong-fen-bu-shi-id-jie-jue-fen-ku-fen-biao-cai-de-keng/">
        </link>
        <updated>2020-10-16T02:55:39.000Z</updated>
        <content type="html"><![CDATA[<h3 id="一-背景">一、背景</h3>
<p>随着业务模式的扩大，多个平台下用户数量不断扩大。并且因为业务需要，需要合并两个业务系统的用户中心，因为需要对接三方系统，所以要求用户的ID标识不能体现出系统的用户量。<br>
技术团队考虑到这种场景以及后续用户的扩大，需要一个方案去解决在<strong>分布式环境</strong>下ID递增的问题，系统自己的ID递增算法也需要做改变。</p>
<h3 id="二-业界比较流行的分布式id解决方案">二、业界比较流行的分布式ID解决方案</h3>
<p>技术团队首先考察业界比较流行的做法，总结不同方案优缺点，然后根据自己的业务来选择更合适的方案。</p>
<h4 id="21-数据库分批id分发">2.1 数据库分批ID分发</h4>
<p>数据库分批ID原理主要是利用分批思想以及乐观锁来解决，目前淘宝中间件TDDL其中的主键分配就是用了这个方案。该方案具体是这样做的：</p>
<ol>
<li>数据库表维护一条数据，记录当前分配ID号以及偏移数量等数据</li>
<li>编写服务，利用乐观锁去CAS更新数据，更新成功就说明拿到了一批ID号，失败的话进行重试，设置一定的重试次数</li>
<li>放入本地缓存或者redis做扣减，如果用完就重复步骤2</li>
</ol>
<p>具体的的表可以是类似这样的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">字段</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">id</td>
<td style="text-align:center">自增ID，没有业务意义</td>
</tr>
<tr>
<td style="text-align:center">current_num</td>
<td style="text-align:center">当前已经分配的ID最大值</td>
</tr>
<tr>
<td style="text-align:center">limit</td>
<td style="text-align:center">一次分配多少个ID</td>
</tr>
<tr>
<td style="text-align:center">version</td>
<td style="text-align:center">版本号，CAS更新用</td>
</tr>
<tr>
<td style="text-align:center">create_time</td>
<td style="text-align:center">记录创建时间</td>
</tr>
<tr>
<td style="text-align:center">update_time</td>
<td style="text-align:center">记录更新时间</td>
</tr>
</tbody>
</table>
<p>比如，limit=1000，一次分配1000个ID，没分配之前current_num = 0，第一次current_num = 1000，第二次依次类推。</p>
<p>当然，上述的方案也有缺点：</p>
<ol>
<li>一条数据，如果有大量流程去更新，要竞争获取行锁，会有性能问题，（可以参考行锁的利弊，丁奇——秒杀场景下MySQL的低效）</li>
<li>强依赖DB，如果当前数据库宕机，导致分布式ID分发服务就会出现问题</li>
<li>如果在主从数据库场景下，需要考虑到主从的延迟性导致分配ID的不一致性</li>
</ol>
<p>所以为了解决上面的问题，笔者参考concurrentHashMap分段锁的思想，将一条记录分段为多个，同时为了避免DB单点可用性，可以将不同的记录分布在不同的数据库上面。</p>
<p>具体的表可以是类似这样的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">字段</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">id</td>
<td style="text-align:center">自增ID，没有业务意义</td>
</tr>
<tr>
<td style="text-align:center">current_num</td>
<td style="text-align:center">当前已经分配的ID最大值</td>
</tr>
<tr>
<td style="text-align:center">limit</td>
<td style="text-align:center">一次分配多少个ID</td>
</tr>
<tr>
<td style="text-align:center">offset</td>
<td style="text-align:center">前后分配的ID间隔数</td>
</tr>
<tr>
<td style="text-align:center">initial_id</td>
<td style="text-align:center">初始自增的ID</td>
</tr>
<tr>
<td style="text-align:center">version</td>
<td style="text-align:center">版本号，CAS更新用</td>
</tr>
<tr>
<td style="text-align:center">create_time</td>
<td style="text-align:center">记录创建时间</td>
</tr>
<tr>
<td style="text-align:center">update_time</td>
<td style="text-align:center">记录更新时间</td>
</tr>
</tbody>
</table>
<p> 新增offset偏移和initial_id字段，offset代表数据库表记录前后分配间隔数量，initial_id代表开始初始的值。<br>
 举个例子，现在将记录分为10条，分布在不同数据库上，那么offset就是10000，limit就是1000，第一条记录第一次开始配合获取0-1000，第二次分配获取10000-11000，依次类推。</p>
<p> 这里需要考虑一个问题，就是如果请求ID分发的服务的流量<strong>怎么路由</strong>到具体的记录呢？如果流量全部都请求到第一条记录上了，就会导致请求不均。<br>
 这个问题很简单，有一定开发经验的读者自然可以联想到用一致性hash去路由，具体路由的key可以根据业务去定，比如用户手机号。（具体一致性hash路由如何实现，可以参考利用treeMap实现）</p>
<p> 以上方案算是解决性能问题，但是还有比较致命的问题，就是无法横向扩展。就比如说现在有10台机器不同数据库，每一个数据库一条记录。假如现在有新的机器10台机器想要加入分配的话，那么就要修改offset和initial_id，所以在不停机实现的话，可能无法实现（目前笔者没有想到方法解决，如果读者有idea可以欢迎和我讨论）。这里笔者提供一种方法去解决：<br>
 提前预先分配100条记录，每一条记录一批次获取100个ID，offset设置为10000，当前10台机器每一台有10条记录，后续有新机器的话直接将记录不停机转移到新机器就可以了。</p>
<h4 id="22-redis序列化分发">2.2 Redis序列化分发</h4>
<p>Redis来生成ID，这主要依赖于Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR和INCRBY来实现。目前单台redis的qps能够达到5W，所以一定程度上能够解决性能问题。具体实现笔者这里就不阐述了。</p>
<p>当然用这个方案也有缺点：</p>
<ol>
<li>依赖redis，如果业务中没有就要依赖这个中间件</li>
<li>生成的ID是单调递增的，容易暴露系统的用户数</li>
</ol>
<h4 id="23-snowflake雪花算法">2.3 SnowFlake雪花算法</h4>
<p>以上2.1 2.2方案都能解决性能问题，但是产生的ID，是连续的，容易暴露自己系统中用户的数量。所以有些系统会要求要趋势递增，而且要保持信息安全。目前雪花算法就能解决这个问题。</p>
<p>其中原理就是：给一个64位的二进制数字，其中</p>
<ul>
<li>第1位置为0。</li>
<li>第2-42位是相对时间戳，通过当前时间戳减去一个固定的历史时间戳生成。</li>
<li>第43-52位是机器号workerID，每个Server的机器ID不同。</li>
<li>第53-64位是自增ID。<br>
<img src="https://zhangyaoo.github.io/post-images/1604663039402.png" alt="" loading="lazy"></li>
</ul>
<p>这个方案也有缺点：</p>
<ol>
<li>实现比较复杂，考验开发人员</li>
<li>需要独立部署实现</li>
<li>强依赖时钟，时钟回拨会有重复的ID</li>
</ol>
<h3 id="三-当前方案是如何做的">三、当前方案是如何做的</h3>
<p> 参考上述方案后，技术团队考虑到未来业务的增加，流量的增长以及后续的扩展，准备利用方案三去实现。其中具体实现可以参考笔者GitHub的实现：<a href="https://github.com/zhangyaoo/fastim/blob/master/fastim-leaf/src/main/java/com/zyblue/fastim/leaf/manager/SnowFlakeManager.java">分布式ID SnowFlake实现</a><br>
 当然使用方案三也会有缺陷，比如会发生时钟回拨问题，以及分布式ID分发服务强依赖zookeeper。</p>
<h4 id="31-时钟回拨">3.1 时钟回拨</h4>
<p> 发生时钟回拨的原因是，如果分发服务正在生成ID的过程中，系统时间因为不可抗拒的因素或者人为因素导致时间倒流了，会导致可能会有重复的ID生成，作为分布式ID分发这个是不准发生的。所以如果发生时钟回拨那么就抛出异常，实现如下</p>
<pre><code class="language-java">long timestamp = System.currentTimeMillis();
// 如果当前时间戳小于上次分发的时间戳
if (timestamp &lt; lastTimestamp) {
    long offset = lastTimestamp - timestamp;
    if (offset &lt;= 5) {
        // 如果时间戳间隔小于5，那么进行等待，等待窗口时间，然后再进行重试
        try {
            wait(offset &lt;&lt; 1);
            timestamp = System.currentTimeMillis();
            if (timestamp &lt; lastTimestamp) {
                throw new RuntimeException(String.format(&quot;lastTimestamp %s is after reference time %s&quot;, lastTimestamp, timestamp));
            }
        } catch (InterruptedException e) {
            logger.error(&quot;wait interrupted&quot;);
            throw new RuntimeException(String.format(&quot;lastTimestamp %s is after reference time %s&quot;, lastTimestamp, timestamp));
        }
    } else {
        // 如果相差过大，直接抛出异常
        throw new RuntimeException(String.format(&quot;lastTimestamp %s is after reference time %s&quot;, lastTimestamp, timestamp));
    }
}
// 如果等于，表明同一时刻
if (lastTimestamp == timestamp) {
    // 如果小于计数器最大值就，增加
    if (this.counter &lt; MAX_SEQUENCE) {
        this.counter++;
    } else {
        // 表明同一时刻，同一机器下，的所有计数器都用完了
        throw new RuntimeException(&quot;Sequence exhausted at &quot; + this.counter);
    }
} else {
    //如果是新的ms开始
    counter = 0L;
}
// 记录这一次时间戳，用作下一次比较
lastTimestamp = timestamp;
// 后续的生成ID逻辑
</code></pre>
<h4 id="32-强依赖zookeeper">3.2 强依赖zookeeper</h4>
<p>以上能解决时钟回拨的问题，但是强依赖zookeeper来生成分布式环境下的当前机器的ID。笔者参考dubbo的设计思想，当ID分发服务通过ID+端口注册到zookeeper的递增持久节点后，返回的节点直接存储再本地文件中，实现高可用，如下</p>
<pre><code class="language-java">/**
 * 高可用，防止zookeeper挂了，本地本机生效
 * 写入本地文件的时机：当前机器获取zookeeper的持久节点后
 */
private void writeWorkerId2Local(int workerId){
    String path = WORKERID_PATH + File.separator + applicationName + File.separator + &quot;workerId.properties&quot;;
    File file = new File(path);
    if(file.exists() &amp;&amp; file.isFile()){
        try {
            FileUtils.writeStringToFile(file, &quot;workerId=&quot; + workerId, false);
        }catch (Exception e){
            logger.error(&quot;e:&quot;, e);
        }
    }else {
        boolean mkdirs = file.getParentFile().mkdirs();
        if(mkdirs){
            try {
                if (file.createNewFile()) {
                    FileUtils.writeStringToFile(file, &quot;workerId=&quot; + workerId, false);
                    logger.info(&quot;local file cache workerID is {}&quot;, workerId);
                }
            }catch (Exception e){
                logger.error(&quot;e:&quot;, e);
            }
        }
    }
}
</code></pre>
<h3 id="四-采用分布式id上线后产生的bug">四、采用分布式ID上线后产生的BUG</h3>
<h4 id="40-问题分析">4.0 问题分析</h4>
<p>当上线分布式ID分发服务后，观察日志，出现大量的报错，如下：</p>
<pre><code>io.lettuce.core.RedisCommandExecutionException: ERR bit offset is not an integer or out of range
at io.lettuce.core.ExceptionFactory.createExecutionException(ExceptionFactory.java:135) ~[lettuce-core-5.2.1.RELEASE.jar:5.2.1.RELEASE]
at io.lettuce.core.ExceptionFactory.createExecutionException(ExceptionFactory.java:108) ~[lettuce-core-5.2.1.RELEASE.jar:5.2.1.RELEASE]
at io.lettuce.core.protocol.AsyncCommand.completeResult(AsyncCommand.java:120) ~[lettuce-core-5.2.1.RELEASE.jar:5.2.1.RELEASE]
at io.lettuce.core.protocol.AsyncCommand.complete(AsyncCommand.java:111) ~[lettuce-core-5.2.1.RELEASE.jar:5.2.1.RELEASE]
at io.lettuce.core.protocol.CommandHandler.complete(CommandHandler.java:654) ~[lettuce-core-5.2.1.RELEASE.jar:5.2.1.RELEASE]
at io.lettuce.core.protocol.CommandHandler.decode(CommandHandler.java:614) ~[lettuce-core-5.2.1.RELEASE.jar:5.2.1.RELEASE]
…………
</code></pre>
<p>乍看是redis的抛出来的错误，和这次分布式ID改造没有什么关系，但是仔细静下来想一想，这次ID改造是从原来的数据库自增改造的，数据库自增数据主键很小是int类型，然后用了64位二进制数据当作ID后，导致redis中用户的数据存不下去，然后笔者就按照这个思路去发掘问题。</p>
<p>笔者从业务抛出的堆栈中发现，这个错误是在判断用户是否是新用户服务方法中抛出的，而判断是新用户的逻辑就是，利用redis的bitmap来解决的：getbit new_user_key userId，其中 userId为用户ID，如果用户发生了交易信息，就会执行：setbit new_user_key userId 1 这个命令。 为什么要用bitmap解决呢，主要是因为如果从交易记录表查询某个用户是否交易信息来判断是新用户的话就会非常耗时，所以只要是发生了交易信息就设置bitmap就可以了。</p>
<p>那为什么会抛出out of range这个错误呢，我们知道bitmap底层其实就是String类型，而String类型的最大长度为512M，官网截图：<br>
<img src="https://zhangyaoo.github.io/post-images/1605157810715.png" alt="" loading="lazy"><br>
所以如果offset超过512M这个范围那么就会抛出异常，512M = 2^9 * 2^12 * 2^12 * 2^3 = 2^32 bit，也就是支持Integer类型的最大值，而我们这次分布式ID服务ID是64位的，支持2^64 bit，所以如果offset也就是userId大于2^32的话就会抛出out of range异常了。</p>
<h4 id="41-问题解决">4.1 问题解决</h4>
<p>找到了问题，也就好去做优化了，这里笔者提供几个方法作为参考</p>
<ol>
<li>在数据库用户表或者用户扩展表中增加一个字段，如果发生了交易，那么将字段标记为老用户</li>
<li>把用户ID放入redis集合set中，利用SMISMEMBER命令判断当前用户是否在集合内，当然这个会有性能问题，其方法时间复杂度位O(N)，官方截图:<br>
<img src="https://zhangyaoo.github.io/post-images/1605162870590.png" alt="" loading="lazy"></li>
</ol>
<h3 id="五-总结">五、总结</h3>
<p> 笔者从分布式ID选型出发，介绍了几种业界几种比较常用的生成方法，并且介绍了其优缺点，然后结合实际业务出发，选择合适的方案。然后介绍了使用SnowFlake算法导致的业务问题，以及分析最后提供解方法。从这次踩坑的经历来说，我们要懂技术体系，并且还要非常熟悉业务，最大程度避免功能之间的相互的影响导致的bug。</p>
<h3 id="六-参考">六、参考</h3>
<ul>
<li>美团分布式算法ID几种实现方式——https://tech.meituan.com/2017/04/21/mt-leaf.html</li>
<li>SnowFlake算法——https://github.com/twitter-archive/snowflake</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SaaS系统多数据源路由优雅解决方案]]></title>
        <id>https://zhangyaoo.github.io/post/saas-xi-tong-duo-shu-ju-yuan-lu-you-you-ya-jie-jue-fang-an/</id>
        <link href="https://zhangyaoo.github.io/post/saas-xi-tong-duo-shu-ju-yuan-lu-you-you-ya-jie-jue-fang-an/">
        </link>
        <updated>2020-10-16T02:26:30.000Z</updated>
        <content type="html"><![CDATA[<h2 id="背景">背景</h2>
<p>在目前的SaaS系统中，业务开发者需要重点关注的一个问题就是数据隔离问题，这个是做SaaS系统必须要考虑的点，多租户数据隔离是每个SaaS系统都要遇到并且要解决的问题，笔者就分享下解决这种问题的思路、具体的解决方案以及优雅的解决思路。</p>
<h2 id="一-解决方案介绍">一、解决方案介绍</h2>
<h3 id="目前业界数据隔离方案">目前业界数据隔离方案</h3>
<p>1、独立数据库，通过动态切换数据源来实现多租户<br>
2、共享数据库，隔离数据架构<br>
3、共享数据库，共享数据表，使用字段来区分不同租户，此方案成本最低</p>
<p>以上方案从上到下，安全性逐渐降低。由于考虑到安全问题，故采用第一种方案解决数据隔离<br>
优点：</p>
<ol>
<li>非常安全</li>
<li>数据互不影响，性能互不影响</li>
<li>数据迁移，数据扩展方便</li>
</ol>
<p>缺点：</p>
<ol>
<li>需要维护大量的数据库</li>
<li>需要自行切换数据库，开发量多且实现复杂</li>
</ol>
<h3 id="具体技术实现">具体技术实现</h3>
<p><strong>简单的架构图</strong><br>
<img src="https://zhangyaoo.github.io/post-images/1603179800053.png" alt="" loading="lazy"><br>
如图所示，SaaS项目大概架构图，关键点是应用层传参，以及路由层的实现。</p>
<p><strong>实现</strong><br>
1、应用层：项目中应用service层是dubbo服务，而且项目分多层，这里需要考虑到多层服务场景下，如何优雅传参问题，如下图所示<br>
<img src="https://zhangyaoo.github.io/post-images/1603181882775.png" alt="" loading="lazy"><br>
我们考虑到租户ID是唯一标识，和业务参数绑定在一起不优雅，所以两种参数分开处理，业务参数直接参数透传，租户ID唯一标识通过隐式传参来处理（参考dubbo http://dubbo.apache.org/zh-cn/docs/user/demos/attachment.html），并且参数记录到服务本地的threadlocal中，以便后续其他业务需要。具体实现如下：<br>
<img src="https://zhangyaoo.github.io/post-images/1603183675071.png" alt="" loading="lazy"></p>
<p>2、路由层：路由层实现主要是自行实现spring框架中DataSource接口，自定义dynamicDataSource类，然后implement DataSource接口，实现getConnection方法。然后重新定义SqlSessionFactory的bean，将自定义DataSource类属性注入。<br>
<img src="https://zhangyaoo.github.io/post-images/1603183148592.png" alt="" loading="lazy"><br>
<img src="https://zhangyaoo.github.io/post-images/1603183156251.png" alt="" loading="lazy"><br>
然后我们只需要关注getConnection方法根据租户ID，选择相对应的租户连接池就可以了。<br>
如图中，我们只需要实现这个selectTenantCodeDataSource()这个方法就可以了，这个方法实现很简单，这里就不贴图了。selectTenantCodeDataSource()方法主要就是从threadlocal中拿租户ID，然后去缓存池map中拿出连接池信息。<br>
<img src="https://zhangyaoo.github.io/post-images/1603183509071.png" alt="" loading="lazy"><br>
其中，dataSourceCachePool是在初始化配置时候，将所有的租户连接池直接创建，然后扔到dataSourceCachePool。key是租户的ID，value是连接池信息。</p>
<p>具体的初始化配置：</p>
<pre><code class="language-java">/**
 * 初始化数据源
 */
@Configuration
public class DataSourceInit {
    
    @PostConstruct
    public void InitDataSource()  {
        log.info(&quot;=====初始化数据源=====&quot;);
        TenantRoutingDataSource tenantRoutingDataSource = (TenantRoutingDataSource)ApplicationContextProvider.getBean(&quot;tenantRoutingDataSource&quot;);
        Map&lt;String, DataSourceCache&gt; dataSourceCachePool = new HashMap&lt;&gt;();

        List&lt;TenantInfo&gt; tenantList = tenantInfoService.InitTenantInfo();
        for (TenantInfo tenantInfo : tenantList) {
            log.info(tenantInfo.toString());
            HikariDataSource dataSource = new HikariDataSource();
            dataSource.setDriverClassName(tenantInfo.getDatasourceDriver());
            dataSource.setJdbcUrl(tenantInfo.getDatasourceUrl());
            dataSource.setUsername(tenantInfo.getDatasourceUsername());
            dataSource.setPassword(tenantInfo.getDatasourcePassword());
            dataSource.setDataSourceProperties(master.getDataSourceProperties());
            dataSourceCachePool.put(tenantInfo.getTenantId(), dataSource);
        }
        //设置数据源
        tenantRoutingDataSource.setDataSources(dataSourceCachePool);
    }
}
</code></pre>
<h2 id="二-方案的隐藏缺点以及解决">二、方案的隐藏缺点以及解决</h2>
<h3 id="隐藏的缺陷">隐藏的缺陷</h3>
<p>相信有一定开发经验的读者应该能想到，上述方案最大的缺点就是性能问题，对MySQL有非常大的影响。因为一开始初始化非常多的连接池，就会占用连接资源，比如租户从100个扩展到了1000个以及更多，那么连接池数量就线性增长，如果一个连接池保持15个活跃连接的话，那么连接数就是15*1000，此时如果MySQL的maxconntion的数量非常小，那么MySQL侧就会抛出”too many connctions“错误，在应用层方面就是MySQL不可用了。<br>
没优化之前的架构：<br>
<img src="https://zhangyaoo.github.io/post-images/1603190851131.png" alt="" loading="lazy"></p>
<h3 id="解决">解决</h3>
<p>想保持数据库分离，又要考虑到MySQL性能问题，只能向连接池优化的方向去考虑，其实可以减少数量就可以了，这里实现方案就是一个数据库实例一个连接池，如下图所示：<br>
<img src="https://zhangyaoo.github.io/post-images/1603190889253.png" alt="" loading="lazy"><br>
具体实现就是将上述方案中的dataSourceCachePool的key改为 “IP+端口”，作为key。然后再数据源路由层，多一层映射（租户ID——&gt;数据库实例）就可以了。</p>
<h2 id="三-更优雅方案解决企业内部开发痛点">三、更优雅方案解决企业内部开发痛点</h2>
<h3 id="现状">现状</h3>
<p><strong>现状</strong>：企业内部项目组开发数据源路由，各个人员开发水平不一，各种路由方案实现不同，自己组内的开发的方案只能自己组内使用，并且实现复杂，耗人力物力。<br>
<strong>目标</strong>：项目组使用直接引入maven包，任何配置都不要配置（自定义的话需要自行在自己项目中配置属性），开箱即用。</p>
<h3 id="具体实现">具体实现</h3>
<p><strong>原理</strong>：直接采用springboot starter开发，将上述方案所有的逻辑和技术实现单独放入springboot starter工程中，采用外部配置的方式实现自定义配置。</p>
<p><strong>开发者实现</strong>：网上有许多springboot starter开发的流程和开发案例，笔者这里就只贴出关键的代码<br>
1、自动装配类：spring.factories中写入这个类DataSourceAutoConfigure，实现bean的自动装入，类里面主要是实现SqlSessionFactory和PlatformTransactionManager，然后在TenantRoutingDataSource的getconnection方法中自定义实现路由逻辑。</p>
<pre><code class="language-java">@Configuration
public class DataSourceAutoConfigure {

    @Resource
    private TenantRoutingDataSource tenantRoutingDataSource;

    @Bean
    @ConditionalOnMissingBean(SqlSessionFactory.class)
    @ConditionalOnBean(TenantRoutingDataSource.class)
    public SqlSessionFactory sqlSessionFactory() throws Exception{
        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();
        sqlSessionFactoryBean.setDataSource(tenantRoutingDataSource);
        Objects.requireNonNull(sqlSessionFactoryBean.getObject()).getConfiguration().setMapUnderscoreToCamelCase(true);
        return sqlSessionFactoryBean.getObject();
    }

    @Bean
    @ConditionalOnMissingBean(PlatformTransactionManager.class)
    @ConditionalOnBean(TenantRoutingDataSource.class)
    public PlatformTransactionManager platformTransactionManager() {
        return new DataSourceTransactionManager(tenantRoutingDataSource);
    }
}
</code></pre>
<p>2、Java SPI机制：利用Javaspi 来获取用户自定义的mybatis plugin。这样做的好处是，不用每次增加一个plugin，就改动数据路由组件的代码。</p>
<pre><code class="language-java">public SqlSessionFactory sqlSessionFactory(@Qualifier(&quot;tenantRoutingDataSource&quot;) TenantRoutingDataSource tenantRoutingDataSource) throws Exception{
        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();
        sqlSessionFactoryBean.setDataSource(tenantRoutingDataSource);
        Interceptor[] plugins = loadMybatisPlugin();
        if(plugins.length &gt; 0){
            sqlSessionFactoryBean.setPlugins(plugins);
        }
        Objects.requireNonNull(sqlSessionFactoryBean.getObject()).getConfiguration().setMapUnderscoreToCamelCase(true);
        return sqlSessionFactoryBean.getObject();
    }

    // SPI机制获取插件
    private Interceptor[] loadMybatisPlugin(){
        List&lt;Interceptor&gt; interceptors = new ArrayList&lt;&gt;();
        ServiceLoader&lt;Interceptor&gt; load = ServiceLoader.load(Interceptor.class);
        load.forEach(interceptors::add);
        return interceptors.toArray(new Interceptor[0]);
    }
}
</code></pre>
<p>3、dubbo filter扩展接口：获取租户ID，并且需要加@Activate注解，这样dubbo在初始化filter链的时候，自动将这个filter注册到filter链中，这样做的好处就是，用户在自己工程中不需要配置filter这个参数，无需增加任何的配置。</p>
<pre><code class="language-java">@Activate(group = {&quot;provider&quot;})
public class TenantCodeContextFilter implements Filter {
    @Override
    public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException {
        String tenantCode = RpcContext.getContext().getAttachment(&quot;tenantCode&quot;);
        TenantCodeContextHolder.setTenantCode(tenantCode);
        return invoker.invoke(invocation);
    }
}
</code></pre>
<p>4、检查用户侧自定义配置是否正确：检查用户的配置是否合理，不合理的话再容器就绪阶段就会抛出异常</p>
<pre><code class="language-java">@Component
public class CheckConfigListener implements ApplicationListener&lt;ApplicationReadyEvent&gt; {

    @Override
    public void onApplicationEvent(ApplicationReadyEvent applicationReadyEvent) {
        ConfigurableApplicationContext applicationContext = applicationReadyEvent.getApplicationContext();
        ConfigurableEnvironment environment = applicationContext.getEnvironment();
        // 检查用户自定义配置是否正确，自行实现
        checkDatasourceConfig(environment);
    }
}
</code></pre>
<p>5、利用缓存池保存多个dataSource对象，一个MySQL实例对应一个dataSource对象，一个dataSource对应多个租户，而不是一个dataSource对应一个租户，这样的好处就是，如果一个MySQL实例里面的租户数据库过多，不会导致一个MySQL实例连接数膨胀问题。</p>
<pre><code class="language-java">    /**
     * 数据源缓存池
     * Key 一个MySQL数据库连接信息key
     * Value 缓存时RDS连接信息与DataSource
     */
    private final Map&lt;String, DataSourceCache&gt; dataSourceCachePool = new ConcurrentHashMap&lt;&gt;();
</code></pre>
<p><strong>用户使用</strong>：直接引入相应的maven，方便快捷</p>
<h2 id="四-todo后续优化">四、TODO后续优化</h2>
<ol>
<li>目前多租户数据源通用工程只支持Dubbo的调用，未来可扩展支持多种协议如HTTP、gRPC</li>
<li>目前只支持Hikari数据源，后续支持多种数据源类型，比如Durid</li>
<li>如果租户数据非常大，可以考虑空间换时间思想，使用缓存存放租户的数据源配置，提升查询效率。</li>
</ol>
<h2 id="参考">参考</h2>
<ul>
<li>SaaS系统数据隔离方案——https://blog.arkency.com/comparison-of-approaches-to-multitenancy-in-rails-apps/</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MySQL联合索引在B+树的存储和查找]]></title>
        <id>https://zhangyaoo.github.io/post/mysql-lian-he-suo-yin-zai-bshu-de-cun-chu-he-cha-zhao/</id>
        <link href="https://zhangyaoo.github.io/post/mysql-lian-he-suo-yin-zai-bshu-de-cun-chu-he-cha-zhao/">
        </link>
        <updated>2020-06-29T09:32:43.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<blockquote>
<p>在对MySQL开发中，联合索引是很常见的一种MySQL优化方式，本文解释了联合索引的存储以及查找过程，可以了解一下底层的原理以及加深对MySQL联合索引的理解。</p>
</blockquote>
<h2 id="innodb-b树">Innodb B+树</h2>
<p>先看一下Innodb B+树的主键索引和辅助索引。这里直接拿张洋大神的图：</p>
<ul>
<li>聚簇索引:<br>
<img src="https://zhangyaoo.github.io/post-images/1593425796639.png" alt="" loading="lazy"></li>
<li>辅助非聚簇索引:<br>
<img src="https://zhangyaoo.github.io/post-images/1593425801180.png" alt="" loading="lazy"><br>
<strong>结构</strong>：当一个表T（id,name,age,sex,high）建一个普通索引  KEY(name)，name的索引结果就和上面辅助非聚簇索引结构一样。<br>
<strong>查询</strong>：当有一个select id,name,age from T where name = &quot;&quot; 辅助索引会根据name在B+树上进行二叉树查找，找出叶子节点数据后发现没有age这个数据，就会进行<strong>回表</strong>操作到主键聚簇索引去查找，拿到聚簇索引叶子节点的age数据。</li>
</ul>
<h2 id="联合索引存储以及寻址">联合索引存储以及寻址</h2>
<ul>
<li>
<p><strong>索引结构</strong>：我们知道上述回表过程也会消耗性能，相当于多查一次，所以系统可以根据业务情况加上一个组合索引，当然并不是一直加组合索引就可以了，因为要考虑到索引存储空间的问题。例如给上述加上一个组合索引  KEY（name,age,sex）【 KEY（col1,col2,col3）】。那么这个组合索引的B+树非叶子节点数据结构和上述辅助非聚簇索引图一样，但是叶子节点是这样的：<br>
<img src="https://zhangyaoo.github.io/post-images/1593425790647.png" alt="" loading="lazy"><br>
叶子节点存储col1,col2,col3这三列数据以及加上ID这一列数据。</p>
</li>
<li>
<p><strong>寻址过程：</strong><br>
例如语句：select * from T where name = &quot;张三&quot; and age=25，先根据name字段从辅助聚簇索引定位到哪一个叶子节点数据中，然后根据age节点在上述表格的前6行中，寻找age= 25的数据，然后找出所有符合的数据以及其对应的ID，然后根据ID来进行回表操作查询。这里返回了三条数据，就回了三次表。<br>
上述回表过程中，笔者引入一个<strong>索引下推</strong>的一个功能，索引下推是MySQL在5.6版本后引入的一个查询优化。就拿上述的例子，在没有优化之前，据name字段查询“张三”后，会拿到6条结果，回表6次，然后从主键索引拿到6条数据后，根据age字段筛选数据；优化之后，先再辅助索引上面根据name字段和age字段筛选符合数据，也就是ID，然后再回表，这里回表了三次。</p>
</li>
<li>
<p><strong>组合索引注意事项</strong><br>
当然，联合索引的最重要的是注意联合索引的使用问题，要遵循最左匹配原则，才可以优化到整个SQL了。</p>
</li>
</ul>
<h3 id="总结">总结</h3>
<p>以上，总结了MySQL的索引的基本原理，以及联合索引的存储和寻址过程，并且引入索引下推概念，还有使用联合索引的注意事项。</p>
<h2 id="参考">参考</h2>
<ul>
<li>MySQL索引背后的数据结构及算法原理——http://blog.codinglabs.org/articles/theory-of-mysql-index.html。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[关于Zookeeper奇数节点以及脑裂问题]]></title>
        <id>https://zhangyaoo.github.io/post/guan-yu-zookeeper-qi-shu-jie-dian-yi-ji-nao-lie-wen-ti/</id>
        <link href="https://zhangyaoo.github.io/post/guan-yu-zookeeper-qi-shu-jie-dian-yi-ji-nao-lie-wen-ti/">
        </link>
        <updated>2020-06-22T02:32:00.000Z</updated>
        <content type="html"><![CDATA[<h3 id="前言">前言</h3>
<blockquote>
<p>Zookeeper作为微服务分布式协调中间件，了解它的原理以及日常开发中的注意事项和可能会出现的问题是有必要的。</p>
</blockquote>
<h3 id="前置知识zab协议">前置知识：ZAB协议</h3>
<p>ZAB：Zookeeper Atomic Broadcast（ZAB）崩坏恢复和原子广播协议<br>
1）崩坏恢复：在master节点宕机情况下，其他集群节点会重新选举master节点，快速领导者选举机制：选举规则会参照最大的分代年龄epoch&gt;最大的事务zxid&gt;server id来进行选举，选举过程就是将自己节点投票信息发给其他集群节点，投票信息附带zxid和serverid，<strong>判断是否超过一半的投票选同一个节点</strong>，那么这个节点就会选举为master。<br>
2）选举完后，就会进行数据同步，将master节点数据同步到slave中，此时对外服务不可用。<br>
3）原子广播：ZAB协议保证消息的一致性和有序性<br>
 一致性：leader发送propasal事务请求（包含zxid），master判断过半机制ack，就认为事务可以提交了，master会提交事务，然后广播提交事务消息，从节点开始提交本事务。一半ack机制，可以看zookeeper是CP，但是不是强一致性；从节点接收propasal后，会将事务写入磁盘。<br>
 有序性：zxid事务id保证全局有序性，每一个slave服务器维持一个FIFO队列，维持局部有序性。</p>
<h3 id="zookeeper脑裂">Zookeeper脑裂</h3>
<p> Zookeeper脑裂都是出现在集群环境中的。指的是一个集群环境中出现了多个master节点，导致严重数据同步和写入问题，数据不一致等等，如果这种情况出现在线上分布式环境下，会导致服务不可用。</p>
<h3 id="出现原因">出现原因</h3>
<p> 可能就是网络环境有问题导致节点之间断开，或者节点假死等等，导致一部分slave节点会重新进入崩坏恢复模式，重新选举新的master节点，然后对外提供事务服务。由于心跳超时（网络原因导致的）认为旧的master死了，但其实旧的master还存活着。</p>
<h3 id="如何解决脑裂">如何解决脑裂</h3>
<p>过半机制，如果集群中某个节点的投票数量大于集群有效节点的一半，就会选出master。这里拿出关键代码：</p>
<pre><code class="language-java">// 验证是否符合过半机制，如果符合就会选举新的master节点
public boolean containsQuorum(Set&lt;Long&gt; set){
    // half是在构造方法里赋值的
    // n表示集群中zkServer的个数（准确的说是参与者的个数，参与者不包括观察者节点）
    half = n/2;
    // set.size()表示某台zkServer获得的票数
    return (set.size() &gt; half);
}
</code></pre>
<p>笔者介绍几种情况，来说明一下几种脑裂的场景</p>
<ul>
<li>
<p>比如集群中有6个节点，一个master和5个slave，分两个机房，每个机房分别三台，发生了机房不可通信的情况，如下图：<br>
<img src="https://zhangyaoo.github.io/post-images/1593419532309.png" alt="" loading="lazy"><br>
然后机房B就会产生新的master，如图<br>
<img src="https://zhangyaoo.github.io/post-images/1593419550068.png" alt="" loading="lazy"><br>
这个时候Zookeeper为了防止这样的情况发生，利用了<strong>过半机制</strong>的这个特性。<br>
上图中，机房B节点为3 小于集群数量的一半，所以，最终上面图中机房B是不会选举出新的master节点的。</p>
</li>
<li>
<p>我们再来看一种情况：比如集群中有5个节点，一个master和4个slave，分两个机房，如下图：<br>
<img src="https://zhangyaoo.github.io/post-images/1593419973725.png" alt="" loading="lazy"><br>
如果发生了机房不能通信的情况，那么机房B因为节点是2个，没有超过一半，就不会产生出新的master节点了。</p>
</li>
<li>
<p>再来看最后一种情况，比如集群中有5个节点，一个master和4个slave，分两个机房，不同的是master节点在机房B，如下图：<br>
<img src="https://zhangyaoo.github.io/post-images/1593420189914.png" alt="" loading="lazy"><br>
如果发生了机房不能通信的情况，那么机房A节点是3个，超过了一半，就会进入崩坏恢复模式产生新的master节点，那么此时集群中就会出现两个master节点了。如下图所示<br>
<img src="https://zhangyaoo.github.io/post-images/1593420219104.png" alt="" loading="lazy"><br>
那么遇到这种情况Zookeeper是如何处理的？答：旧的leader所有的写请求同步到其他followers节点是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。这个时候旧的master进入恢复模式进行数据同步。<br>
所以按照上面的情况，机房A的所有followers节点正常通信，机房B的所有节点重新进入恢复模式进行数据同步。</p>
</li>
</ul>
<p>总结：通过Quorums机制来防止脑裂，当leader挂掉之后，可以重新选举出新的leader节点使整个集群达成一致；当出现假死现象时，通过epoch大小来拒绝旧的leader发起的请求，当出现这种情况，旧的leader 进入恢复模式进行数据同步。</p>
<h3 id="引出奇数节点">引出奇数节点</h3>
<p> 知晓以上场景后，我们知道，2台机器也能选举出master，只不过只要有1个死了zookeeper就不能用了，因为1没有过半。所以2个zookeeper的死亡容忍度为0。同理，要是有3个zookeeper，一个死了，还剩下2个正常的，过半了，所以3个zookeeper的容忍度为1。如果按照这样的机制推理，那么得出2-&gt;0;3-&gt;1;4-&gt;1;5-&gt;2;6-&gt;2  左边是数量，右边是容忍度，所以2n和2n-1的容忍度是一样的，所以可以得出，集群是<strong>奇数个能够节省资源</strong>。</p>
<!--下面的奇数节点的作用需要确认 TODO-->
<h3 id="总结">总结</h3>
<p>以上，笔者总结了ZAB协议，到Zookeeper防止脑裂的场景以及如何处理，以及结合例子，Zookeeper集群在奇数节点下的作用。</p>
<h3 id="参考">参考</h3>
<ul>
<li>ZooKeeper集群的脑裂问题——https://www.cnblogs.com/shoufeng/p/10591526.html</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[金融级业务下分布式事务保证数据一致性]]></title>
        <id>https://zhangyaoo.github.io/post/jin-rong-ji-ye-wu-xia-fen-bu-shi-shi-wu-bao-zheng-shu-ju-yi-zhi-xing/</id>
        <link href="https://zhangyaoo.github.io/post/jin-rong-ji-ye-wu-xia-fen-bu-shi-shi-wu-bao-zheng-shu-ju-yi-zhi-xing/">
        </link>
        <updated>2020-06-22T02:27:01.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<blockquote>
<p>随着分布式服务架构的流行与普及，原来在单体应用中执行的多个逻辑操作，现在被拆分成了多个服务之间的远程调用。微服务化后，随着带来的服务之间的分布式事务问题，尤其是在金融业务下，分布式事务是保证数据一致性的重要保证。本文着重会讲分布式事务场景和业界主流的解决方案。</p>
</blockquote>
<h2 id="一-引入">一、引入</h2>
<p> 资金转账在金融业务下是一个非常重要而且常见的场景，如果因为技术问题导致资金转账错误，导致数据不一致问题，那么就会造成无法预测的后果。<br>
 笔者这里拿银行转账的例子来说（这里的转账有很多场景比如银行卡之间充值提现、银行账户之间的转账等等），比如甲银行账户A向乙银行账户B转账1W：</p>
<p>同步调用：</p>
<ol>
<li>A银行对转出账户执行检查校验，进行账户金额扣减。</li>
<li>A银行同步调用B银行转账接口。</li>
<li>B银行对转入账户进行检查校验，进行账户金额增加。</li>
<li>B银行返回处理结果给A银行。<br>
<img src="https://zhangyaoo.github.io/post-images/1593745125729.png" alt="" loading="lazy"><br>
同步调用问题：</li>
</ol>
<ul>
<li>如果B银行因为网络原因导致接口不通，那么A调用线程会长时间阻塞。</li>
<li>如果A扣减后，发送请求后，在网络中丢失了，B银行没有收到请求，导致账户A扣减了，账户B没有加</li>
<li>如果账户B扣减成功了，由于某种原因比如网络异常没有及时回调给甲银行，那么账户A就认为是异常请求，则会回滚事务，导致数据不一致。</li>
</ul>
<p>再来看一下异步调用：</p>
<ol>
<li>A银行对转出账户执行检查校验，进行账户金额扣减。</li>
<li>主线程将请求数据异步写入队列MQ</li>
<li>真正消费者程序对B银行进行远程调用</li>
<li>B银行对转入账户进行检查校验，进行账户金额增加。</li>
<li>B银行返回处理结果给A银行。<br>
<img src="https://zhangyaoo.github.io/post-images/1593743096206.png" alt="" loading="lazy"><br>
异步调用问题：</li>
</ol>
<ul>
<li>如果账户A扣减本地事务成功了，但是消息发出后，因为网络原因或者其他宕机原因，导致消息未发送成功，没有进行B账户远程调用，导致本地事务和消息不一致性。</li>
<li>MQ消费端程序如果消费消息成功，请求银行成功了，但是回传ACK给MQ失败了，那么回导致消费端程序重复消费问题，那么就会出现重复转账的问题。</li>
</ul>
<p>异步调用解决了同步调用的主线程阻塞问题，但还是没有解决数据一致性问题。而且引入MQ中间后，还要考虑到本地事务和MQ消息一致性问题，还有其他的引入后的维护工作，比如消息丢失，消息重发等等问题。</p>
<h2 id="二-分布式事务解决方案">二、分布式事务解决方案</h2>
<p> 讲到了分布式事务，自然离不开分布式系统的一些基本原则和定理：CAP原则和BASE理论，相信读者应该都知道，这里不做过多阐述。业界根据这些规则和理论，衍生出了各种分布式事务解决方案：XA规范，2PC，3PC，本地消息表方案，基于消息中间件的最终一致性方案，TCC方案，阿里的SEATA，SAGA方案和最大努力通知等等。<br>
 以上每个方案都有自己的应用场景，就拿2PC来说，MySQL的事务型日志redolog二段提交（redolog(prepare)--》binlog--》redolog(commit)）保证binlog和redolog数据一致性，Zookeeper的proposal事务二段提交（半数以上ack返回成功表示写入数据成功）保证leader和foller的数据一致性，这些都是2PC的应用。<br>
 金融场景下类似资金业务需要保证最终一致性解决分布式事务，不需要保证转账实时性。所以本地消息表、基于MQ中间件的最终一致性等柔性方案是首选的方案。这些基于消息的分布式事务，本质上就是，本地事务+从事务，从事务从消息中获取信息进行本地提交，这里保持<strong>异步事务机制、只能保证最终一致性</strong>。</p>
<h3 id="21-利用本地消息表思想解决一致性问题">2.1 利用本地消息表思想解决一致性问题</h3>
<p> 一般来说，跨行转账的原理，会存在一个中国人民银行的中间人角色来操作转账，但不在本次讨论的范围内。<br>
 业界银行转账大部分都是同步转账，异步获取转账结果，包括第三方支付平台对接银行都是这样玩的。这里笔者就利用本地消息表思想来具体叙述数据一致性是如何保证的，老规矩先放图：<br>
<img src="https://zhangyaoo.github.io/post-images/1603274563955.png" alt="" loading="lazy"><br>
其中交易记录表大概长这个样子：</p>
<table>
<thead>
<tr>
<th style="text-align:center">字段</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">id</td>
<td style="text-align:center">自增ID，没有业务意义</td>
</tr>
<tr>
<td style="text-align:center">trade_order_num</td>
<td style="text-align:center">交易订单号，作为转账记录唯一标识</td>
</tr>
<tr>
<td style="text-align:center">source_account_num</td>
<td style="text-align:center">交易转出方账户ID</td>
</tr>
<tr>
<td style="text-align:center">target_account_num</td>
<td style="text-align:center">交易收款方账户ID</td>
</tr>
<tr>
<td style="text-align:center">status</td>
<td style="text-align:center">状态机，0=预创建，1=转账中，2=转账成功，3=转账失败</td>
</tr>
<tr>
<td style="text-align:center">pay_success_time</td>
<td style="text-align:center">记录转账成功时间</td>
</tr>
<tr>
<td style="text-align:center">create_time</td>
<td style="text-align:center">记录创建时间，可作为窗口时间内判断标准</td>
</tr>
<tr>
<td style="text-align:center">update_time</td>
<td style="text-align:center">记录 更新时间</td>
</tr>
</tbody>
</table>
<p>账户表大概长这个样子：</p>
<table>
<thead>
<tr>
<th style="text-align:center">字段</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">id</td>
<td style="text-align:center">自增ID，没有业务意义</td>
</tr>
<tr>
<td style="text-align:center">account_num</td>
<td style="text-align:center">账户ID</td>
</tr>
<tr>
<td style="text-align:center">current_amt</td>
<td style="text-align:center">当前账户余额</td>
</tr>
<tr>
<td style="text-align:center">lock_amt</td>
<td style="text-align:center">冻结金额，用来记录临时状态的核心转账数据 。真实余额=current_amt-lock_amt</td>
</tr>
</tbody>
</table>
<p>图中的步骤大致分为8步，这里细致讲一下每一步的详细步骤，分别是：</p>
<ol>
<li>插入初始状态的交易数据。 这一步骤的目的是保证发起同步转账请求和本地初始事务一致性，还有一个目的就是生成转账记录唯一标识，用来标识本次转账</li>
<li>同步发起转账请求，带上唯一标识以及其他的业务参数</li>
<li>银行乙会校验参数信息，并且同步返回转账通知，类似“我接收到了你的请求了，我还有其他事情，我等会返回结果给你”</li>
<li>会根据同步的转账通知来判断这次交易是否合法，然后会记录结果到交易表中。这里并且需要冻结账户的一部分金额，作为临时中间态数据。并且需要更新本地交易表状态为转账中</li>
<li>银行乙需要记录本次交易记录，插入一条交易中的数据，直至回调转账结果给银行甲，将这条记录置为转账成功。并且银行乙自己生成的收款交易流号，然后放入到回调结果中，传给银行甲</li>
<li>异步回调结果给银行甲，其中回调参数重要的有银行甲的唯一的转账标识，还有银行乙自己生成的收款交易流号</li>
<li>根据异步回调的状态，更新交易状态数据，如果成功，那么会扣减账余额，并且释放冻结金额，如果失败，直接释放冻结金额。此时算是正常的一条<strong>流程闭环</strong>走完</li>
<li>当然不是所有的业务能够正常走完流程闭环，也会出现各种原因导致不能走完。为了保证数据一致性，会增加一个补偿程序，定时去拉取异常数据，异常数据指的是交易状态为0和1并且<strong>不在正常窗口业务</strong>时间内的数据（0和1属于中间态，而2或者3数据终态），窗口时间指的是正常业务从开始到结束的时间。</li>
</ol>
<ul>
<li>如果异常数据状态是0，那么表示有可能是本地更新事务失败了，也有可能是请求或者返回在网络中丢失了，补偿程序里面会根据本地的表数据判断是哪个步骤除问题了，就比如说本地数据没有银行乙的交易流水号，那么就是网络出问题了，后面就可以进行补偿操作</li>
<li>如果异常数据状态是1，那么表示可能是银行乙接口有问题或者网络有问题原因导致没有及时回调，这个时候补偿程序就用银行乙的交易流水号是去查询交易是否完成，然后更新自己本地的数据。</li>
</ul>
<p>以上流程是一次正常的交易过程，当然不是所有的交易流程都是这样走的，不过大部分转账流程和上述步骤相类似，其中的细致步骤在每个交易系统中略有不同。</p>
<p>总结起来，利用本地消息表思想能够解决上面第一部分文章的同步调用的缺点，能够解决第二个第三 个问题，但是第一个问题就无法解决了，只要是同步调用都会出现这个问题，不过有其他方式去解决这个问题。以笔者认知来说，银行转账的业务都是同步调用的。出现接口阻塞这个问题，需要设置超时时间，如果超过超时时间，就记录下这条交易，异步放入<strong>重试队列</strong>，一段时间后进行<strong>重试</strong>。</p>
<h3 id="22-事务消息解决本地事务和mq消息一致性问题">2.2 事务消息解决本地事务和MQ消息一致性问题</h3>
<p> 转账业务，如果用异步的话，当出现MQ问题或者消费也者程序出现消息挤压或者消费者端出现问题 的话，那么整个业务时间线会拉的非常长。所以笔者认为异步不适合这种业务，异步<strong>本质上</strong>是对下游服务的一个缓冲，适合在自己系统中使用，不适合跨系统或者三方调用。当然不是所有的场景都不合适，如果流量非常大话，对方系统有限流机制，使用MQ也算是一种解决方案，这还是看具体业务。</p>
<p> 什么样的场景适合使用MQ？一般来说在需要限流削峰、异步解耦等场景使用，所以还是拿上面的图，图中标框的部分业务适合用MQ来解决<br>
<img src="https://zhangyaoo.github.io/post-images/1603790962781.png" alt="" loading="lazy"></p>
<p> 如果用MQ来做的话，那么会有如下图步骤：<br>
<img src="https://zhangyaoo.github.io/post-images/1603792233049.png" alt="" loading="lazy"><br>
图中的步骤大致分为6步，这里细致讲一下每一步的详细步骤，分别是：</p>
<ol>
<li>消息生成者发送消息，broker接受消息</li>
<li>MQ broker收到消息，随即将消息进行持久化，并且存入库。这一步是防止MQ因为物理原因宕机导致的消息丢失，并且入库的时候要判断幂等，防止没有及时返回ack，导致生产者重发消息。</li>
<li>返回ACK给生产者。如果不及时返回或者长时间没有返回，生产者会认为这条消息发送失败，会重新发送。</li>
<li>MQ push消息给对应的消费者或者消费者主动来pull消息，然后等待消费者返回ACK</li>
<li>如果消息消费者在指定时间内成功返回ack，那么MQ认为消息消费成功，在存储中删除消息，即执行第6步；如果MQ在指定时间内没有收到ACK，则认为消息消费失败，会尝试重新push或者pull消息,重复执行4、5、6步骤</li>
<li>MQ删除消息</li>
</ol>
<p> 以上为一条正常的消息从生产到消费的过程，每一步都是不可或缺的。而且我们可以看到，当引入中间件MQ后，消费端业务需要保持幂等。</p>
<p> 回到上面文章最开始的部分，并没有解决异步调用问题一。没有彻底解决本地事务和消息不一致性。所以这个时候，就需要事务消息解决本地事务和MQ消息一致性问题了，笔者重新画了一张图来说明一下事务消息是如何做的：<br>
<img src="https://zhangyaoo.github.io/post-images/1603873494446.png" alt="" loading="lazy"><br>
图中的步骤大致分为几步，分别是：</p>
<ol>
<li>生产者发送一条prepare消息</li>
<li>MQ接受到消息后，先进行持久化,状态为待确认的消息</li>
<li>返回ACK给消息生产者</li>
<li>执行本地事务：扣减账户余额，插入交易流水。如果这个事务执行失败，那么相当于业务执行失败，抛给用户交易失败</li>
<li>执行完成后，将结果发送执行结果给MQ</li>
<li>根据结果将消息commit或者rollback  commit：将消息状态置为已确认  rollback：将消息删除</li>
<li>采用pull或者push消费已确认的消息 ，后面流程大致和普通的流程都一样</li>
</ol>
<p> 这里还没有体现另外一个流程，就是如果消息待确认状态在一定时间内没有转换为已确认，那么MQ会回查本地事务执行状态是否成功。这个是为了保证在第五步发送的消息在网络中丢失或者消费者宕机等情况下，能够回滚。<br>
 以上是事务消息大致的流程，能够解决本地事务和MQ消息一致性问题，这里强调的是，不管是是事务消息还是普通的消息，消费端都需要做幂等处理。<br>
 总结来说，<strong>ack+补偿+重试+幂等</strong>是保证一致性的关键。</p>
<h4 id="221-事务消息常见问题">2.2.1 事务消息常见问题</h4>
<ol>
<li>如果consumer消费失败，是否需要producer做回滚呢？<br>
不需要，MQ作用要保证的就是<strong>最终一致性</strong>，如果consumer消费失败，就让它进行重试直至成功。如果重试超过一定次数的话，那么就人工介入。</li>
</ol>
<h2 id="三-其他方式保证数据一致性">三、 其他方式保证数据一致性</h2>
<p> 当然，保持数据一致性不光是分布式事务来保证，业务上还要配合其他的辅助来保证，这里笔者就列举几种</p>
<ol>
<li>全链路幂等<br>
全链路幂等保证不产生脏数据，保护核心流程正常执行。</li>
<li>重试机制<br>
对异常业务进行重试，超过指定重试次数仍失败的进行人工介入。</li>
<li>业务对账<br>
业务内部准实时对账，比如业务发生后充值提现，对比用户余额是否正确，用户业务流水是否正确。<br>
T+1日对账，程序或者人工定时扫描核心业务数据，保证当日数据准确。对账后自动检测并且修复重试业务</li>
<li>业务指标监控<br>
监控数据库中的订单预占资金没有释放，状态机是不是最终态监控，单位窗口时间内业务状态是否异常，账户中的预扣减金额是否释放，业务重试次数是否超过阈值等等业务监控。</li>
</ol>
<h2 id="四-总结">四、总结</h2>
<p> 分布式场景，要用分布式的思维去思考问题。要考虑任何的超时，断电，维护不同物理存储的数据的可能存在的状态不一致的场景，说白了要面向失败编程。</p>
<h2 id="五-参考">五、参考</h2>
<ul>
<li>有赞出金系统——https://tech.youzan.com/build-a-withdraw-sys/</li>
<li>分布式事务的思考——https://www.cnblogs.com/sujing/p/11006424.html</li>
<li>阿里云RocketMQ文档——https://help.aliyun.com/document_detail/43348.html</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[O(N)时间复杂度下，二进制反转]]></title>
        <id>https://zhangyaoo.github.io/post/er-jin-zhi-fan-zhuan/</id>
        <link href="https://zhangyaoo.github.io/post/er-jin-zhi-fan-zhuan/">
        </link>
        <updated>2020-06-19T08:11:36.000Z</updated>
        <content type="html"><![CDATA[<h4 id="题目描述">题目描述</h4>
<p>给定一个32位整数 . 输出二进制表示反转后的值.<br>
例如 input 43261596（二进制 00000010100101000001111010011100）<br>
返回 output 964176192（二进制 00111001011110000010100101000000）</p>
<p>目前笔者就想到了时间复杂度在O(N)的解决思路：</p>
<ol>
<li>循环判断输入数据的低位是0还是1，具体判断方法是和1进行与操作</li>
<li>如果判断是，返回的结果+1，不是1那么不做任何处理</li>
<li>每次循环，input的数据向左移一位，output数据向右移动一位</li>
<li>循环32次，返回结果</li>
</ol>
<pre><code class="language-java">/**
    *  二进制数据反转
    */
public class BitReverse {

    public static int reverse(int n) {
        int result = 0;
        for (int i = 0; i &lt; 32; i++) {
            result = result &lt;&lt; 1;
            if ((n &amp; 1) == 1) {
                result++;
            }
            n = n &gt;&gt; 1;
        }
        return result;
    }

    public static void main(String[] args){
        System.out.println(reverse(1&lt;&lt;30));
        System.out.println(1&lt;&lt;30);
    }
}
</code></pre>
]]></content>
    </entry>
</feed>